\section{Prompting techniques}

Prompting techniques encode human priors, making it difficult to assess a language model's intrinsic reasoning abilities \cite{wang2024chainofthoughtreasoningprompting}

\subsection{Prompt engineering}
Motivation for prompt engineering is to improve the model's capabilities not by changing the underlying weights with training on data but by crafting an optimal instruction string, or a prompt.
This can be done by providing examples of the task as a part of the prompt or by instructing the model how to solve the task.
In its essence, the model is a left-to-right text completion engine. We can make the analogy with human thinking modes, where it is said that humans
have a fast automatic "System 1" mode and a slow and deliberate "System 2" mode \cite{yao2023treethoughtsdeliberateproblem}. 
With a good prompt we can shift the model from "System 1" to "System 2".
\subsubsection{In-context learning}
Prompts are distinguished based on the number of included examples.
\begin{table}[h!]
    \centering
    \begin{tabular}{|c|p{12cm}|}
    \hline
    \textbf{Prompting Type} & \textbf{Description} \\
    \hline
    Zero-shot Prompting & Prompt has no examples, model relies on its pre-trained knowledge. \\
    \hline
    One-shot Prompting & Prompt has one example to guide the model. \\
    \hline
    Few-shot Prompting & Prompt includes a few examples (typically 2 to 5). \\
    \hline
    \end{tabular}
    \caption{Comparison of Zero-shot, One-shot, and Few-shot Prompting}
\end{table}        
Research\cite{brown2020languagemodelsfewshotlearners} has shown that with growing model size the knowledge-generalizing ability of the model increases. Instead of expensive fine-tuning
models can reuse knowledge from pre-training and solve many tasks when provided just by a few examples.
\subsection{Prompting techniques}

\subsubsection{Chain-of-thought (CoT)}
\input{literature/chain-of-thought}

\subsubsection{Reflexion}
Reflexion converts binary or scalar feedback from the environment into verbal feedback in the form 
of a textual summary, which is then added as additional context for the LLM agent, e.g. CoT or ReAct module, in the next episode. \cite{shinn2023reflexionlanguageagentsverbal}

Reflections go into a long-term memory context limited to a sliding window with maximum capacity. \cite{shinn2023reflexionlanguageagentsverbal}

Improves performance over strong baselines on sequential decision making, reasoning and programming tasks. \cite{shinn2023reflexionlanguageagentsverbal}
\subsubsection{ReAct}
Multi-turn prompting technique that forms the basis of agentic LLMs. The model is given a set of tools, such as a Wikipedia search function or a math expression evaluator.
The model can go through several steps of using the tools, which generates "observation". The model uses these observations to generate a final answer and leaves the ReAct chain when ready using a "finish" function.
\subsubsection{Tree-of-thought}
Similar to CoT with self-consistency but the reasoning chain is split up into steps creating a tree. This reasoning tree can then be explored using a graph search algorithm such as DFS.
