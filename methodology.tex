\section{Inference framework}
Taking inspiration from DSPy\cite{khattab2023dspycompilingdeclarativelanguage}, we first implement a simple LLM-calling framework 
capable of invoking several selected inference strategies. Motivations for this are twofold:
\begin{enumerate}
    \item DSPy is a young and ambitious project aiming at simplifying LLM pipeline design and optimization. 
    As we focus on single-stage prompt program optimization, this capability is not useful for our work. 
    Furthermore, due to the framework's infancy, it lacks proper documentation and sometimes exhibits unexpected behavior.
    \item Implementing the prompting techniques discussed in \ref{sec:inference} provides further insight into their workings and performance.
\end{enumerate}

\subsection{Structured generation}
Following current research trends\cite{zhang2025metapromptingaisystems}, we build our inference framework around a structured JSON template,
or a \texttt{Signature}. The \texttt{Signature} structure consists of input and output fields and additional instructions. 
These fields are populated by a \texttt{Field} data structure.
Of particular interest are the output fields, which hold the output name, desired type and optional description. 

Interactions with LLMs in a structured format benefit from better predictability. By implementing the \texttt{Signature} structure,
we can use LLMs as we would a function in any programming language. Functions in programming languages also have functions signatures which
specify input and output names and types.

When employing good naming practices the model can often deduce the task only by looking at output names and types.
Consider the simple \texttt{Signature} in figure \ref{box:simplesig}, which implicitly instructs the LLM to return a word with a meaning opposite to the provided input.

\begin{figurebox}{Simple Signature}{box:simplesig}
    \hlbox{ctuorange}{Word: \texttt{str}}  
    \hlbox{brown}{Antonym: \texttt{str}}
\end{figurebox}

For more complex tasks, filling the output descriptions or even adding explicit instructions is necessary.
In figure \ref{box:complexsig}, notice that it is possible to specify multiple inputs and outputs, which are then generated in the order given.

\begin{figurebox}{Complex Signature. }{box:complexsig}
    \hlbox{ctuorange}{Text: \texttt{str} (Student text)\\
    Grading guide: \texttt{str} (Steps to follow during evaluation)} 
    \hlbox{brown}{Evaluation: \texttt{str} (Textual feedback)\\
     Grade: \texttt{int} (Numerical grade 1-10)} 
    \hlbox{ctublue}{Grade the text.\\ You are an expert text evaluator. \\
    Use the grading guide to evaluate the test and give a final grade. 
    Use formal language and Markdown formatting in the evaluation\\ and output a 1-10 integer for the grade.}
\end{figurebox}

We will maintain this formatting style whenever showing a \texttt{Signature} structure in the future: \textcolor{ctuorange}{orange inputs}, \textcolor{brown}{brown outputs} and the optional \textcolor{ctublue}{blue instructions}.

Sufficiently large instruction-tuned LLMs are usually good at reliably producing JSON output.
For smaller models or more complex output structures, it might be necessary to use some form of constrained generation as discussed in \ref{sec:inference}.
A JSON schema could be constructed automatically from the \texttt{Signature} and passed into a parser-based sampler.
However, this is not necessary for our use-case and the only safeguard we implement is repeated generation in case of a parsing error.

\subsection{Predict method}
To facilitate \texttt{Signature}-powered generation, we implement a \texttt{predict} method that 
involves 1) prepending a developer prompt to the messages, and 2) parsing of \texttt{Signature} outputs.

\begin{figurebox}{Predict method developer prompt with highlighted prompt sections: directive, context, examples and format specifications.}{box:predictdev}
    \hlbox{ctulightblue}{You are an intelligent function that returns structured JSON outputs matching a given schema.
    }
    
    \hlbox{ctulightblue}{
    You will receive a JSON object containing: \\
        - `inputs`: a dictionary of named inputs \\
        - `outputs`: a dictionary specifying the expected output fields with their types and descriptions\\
        - `instructions`: a task or question to answer (optional)\\

    Your job is to:\\
        1. Understand the task from `instructions` or infer it from `inputs` and `outputs`\\
        2. Use the `inputs` to compute or generate the answer\\
        3. Respond **only** with keys from the `outputs` dictionary and values matching the described types
    }
    \hlbox{ctulightblue}{Only return a flat JSON object like:\\
    \{
    "field1": <value matching type and description>,\\
    "field2": <...>
    \}}
    
    \hlbox{ctulightblue}{Do not add metadata, explanations, or wrap outputs in additional structures.\\
    Do not include type names or field descriptions in the output.\\
    Your output must be strictly valid JSON and fill **all** requested output fields.}
\end{figurebox}

The developer prompt has to clearly explain to the LLM how to work with the JSON-based \texttt{Signature}.
In figure \ref{box:predictdev} notice the sections of the prompt following prompt engineering principles outlined in \ref{sec:preng}.

First, the directive states the task, then further context is added about the \texttt{Signature} data structure and the task.
Next, notice the example showing the proper output. Finally, few more clarifying instructions about the output format are added.
In experiments, this prompt is successful in incentivizing parsable outputs adhering to the \texttt{Signature} specifications.

Parsing the output presents some challenges as the LLM sometimes wraps the JSON output into a Markdown code block
or uses inconsistent escape sequences. We implement a simple parser based on regular expressions that is able to parse 
the majority of outputs. In case of a parsing issue or model failure, such as getting stuck in a generation loop, we add a repeated generation
feature.

\subsection{Inference techniques implementation}
Leveraging the \texttt{predict} method and the modular \texttt{Signature}-based interface, we implement a suite of inference-time prompting techniques. 
Each technique is realized through systematic modifications of the \texttt{Signature} fields, changing the developer prompt and the chaining of multiple generation steps 
and function calls. This design allows for modularity and reuse while preserving transparency.
We implement the following methods.
\begin{enumerate}
    \item \textbf{Chain-of-thought}\cite{NEURIPS2022_8bb0d291}: Prepends a reasoning field to the \texttt{Signature} outputs which forms a scratch pad for the LLM.
    \item \textbf{Chain-of-thought with Self-consistency}\cite{wang2023selfconsistencyimproveschainthought}: Multiple CoT generations with majority-voting.
    \item \textbf{ReAct}\cite{yao2023reactsynergizingreasoningacting}: Adding tools allows the LLM to interleave thoughts and action steps.
    \item \textbf{Program-of-thought}\cite{chen2023programthoughtspromptingdisentangling}: Two-stage CoT with Python-code execution
    \item \textbf{Reflexion}\cite{shinn2023reflexionlanguageagentsverbal}: After an initial generation, the model is prompted to self-critique and revise its output.
    \item \textbf{Tree of Thoughts}\cite{yao2023treethoughtsdeliberateproblem}: The problem is first decomposed and each step is expanded, forming a thought tree, which is then traversed with BFS or DFS.
\end{enumerate}

These techniques are however not the main focus of this bachelor's thesis, and we proceed without further discussion or evaluation.
In our prompt optimization method, we will utilize only the basic Chain-of-thought module.

\section{Datasets}
In this section, we discuss choosing datasets for testing our method and comparing various prompt optimization approaches. 
While searching available datasets, we focus on the following criteria:
\begin{enumerate}
    \item \textbf{Output complexity}: We focus on more complex outputs. Specifically, datasets with multiple-choice or Yes/No answers are omitted. 
    This disqualifies commonly used datasets as MMLU or BigBenchHard.
    \item \textbf{Contamination}: Recently, researchers have expressed concern\cite{white2025livebenchchallengingcontaminationlimitedllm} 
    whether benchmarks are reliable evaluations of models as they might appear in their training data. We omit most common datasets, such as GSM8k\cite{cobbe2021gsm8k}, which has been shown to have inflated scores for some models\cite{testing_language_models_on_a_held_out_high_school_national_finals_exam}.
    \item \textbf{Output verification}: We prefer to use simple automatic verification rather than using LLM-as-a-judge, which has been shown to be 
    biased in some circumstances\cite{ye2024justiceprejudicequantifyingbiases}. Neither do we use human feedback, which defeats the purpose of automatic prompt optimization.
    \item \textbf{Difficulty}: We omit tasks where models already have near-perfect score. 
    \item \textbf{Benefit from non-trivial instruction}: We focus on tasks where helpful hints and step-by-step tutorial-like instructions might increase the probability of successful solution.
\end{enumerate}
We now list the datasets that we will use for evaluation and explain why they were chosen.
\subsection{Livebench}
The Livebench\cite{white2025livebenchchallengingcontaminationlimitedllm} dataset is very recent and has been created with the issue of data contamination in mind.
It also addresses the issues of LLM-as-a-judge verification and all its categories can be verified automatically. It is also very challenging, with top models achieving $65\%$ accuracy\cite{white2025livebenchchallengingcontaminationlimitedllm}.

Out of the tasks available in Livebench, we select the \texttt{Connections} task from the \texttt{Language} subset. 
This task consists of sorting given words into non-trivial groups of four based on semantics, phonetics and other features. 
An ideal prompt would attempt to list multiple possible aspects based on which the words can be sorted and also include a helpful example.

\subsection{Code Contests}
Programming puzzles are a difficult and easily verifiable task. Although \texttt{CodeContests}\cite{li2022competition} is an older dataset, 
we anticipate this dataset presents reduced contamination risks compared to datasets with simpler outputs. With LLM-powered coding assistants on the rise, we
feel this is a relevant application area for our method. 

\subsection{Sequences}
We design a small but challenging dataset based on predicting the next number in an integer sequence.
Each sequence is created according to a formula with randomly selected coefficients. The formulas fall into several categories, for example
\begin{itemize}
    \item \textbf{Linear with modulo}: $s(i) = \operatorname{mod}_{q}(a_{1} i + b_{1})$
    \item \textbf{Sum}: $s(i) = \sum_{j=0}^{i-1}a_{1} j + b_{1}$
    \item \textbf{Alternating}: $s(i) = a_{1}i + a_{2}i(-1)^{i}$.
\end{itemize}
This tests the model's ability to 1. detect and understand patterns and 2. systematically perform simple arithmetic. 
In practice, we will optimize just for a single sequence category and observe whether the optimizer evolves a prompt with a tutorial for the specific sequence category.
Experiments showed that the \texttt{Alternating} class of sequences has a good difficulty balance, and we will use it for evaluation.

\section{Evaluation Metrics}
The evaluation metric defines the optimization goal and thus forms its central component. 
Most evaluation metrics are task-specific and divisible into two categories based on whether they are used in a supervised or self-supervised context.
\subsection{Metrics for Supervised Optimization}
Supervised optimization is supported by gold labels and its underlying metrics all perform comparisons between the results and the gold labels.
These include classification metrics like accuracy or Hamming Loss, regression metrics like Mean Squared Error, and many others.

All three main benchmarks that we will use (\texttt{Connections}, \texttt{CodeContests}, \texttt{Sequences}) 
fall into this category. For each benchmark we use a simple accuracy metric. Given a dataset $\mathcal{D}$ and questions $q$ and gold labels $g$, $(q,g) \in \mathcal{D}$:
\begin{enumerate}
    \item \textbf{Connections}:  $\mathcal{F}_{\mathcal{D}_{\text{Conn}}}(q, g) = \operatorname{Overlap}(\operatorname{Groups}(q), \operatorname{Groups}(g))$
    \item \textbf{CodeContests}: $\mathcal{F}_{\mathcal{D}_{\text{Code}}}(q, g) = \operatorname{FinishesExecution}(q) + \operatorname{PassesAllCases}(q, g)$
    \item \textbf{Sequences}: $\mathcal{F}_{\mathcal{D}_{\text{Seq}}}(q, g) = \operatorname{Equals}(q, g)$
\end{enumerate} 

\subsection{Metrics for Self-Supervised Optimization}\label{sec:ssometrics}
In self-supervised contexts, metrics are usually based on reward models pretrained on human preference or environment data.
To allow our method to be applied to gold label-free problems, we turn to LLM-based direct pairwise comparisons.
Given a dataset $\mathcal{D}$ with queries $q$, output $y$ produced by prompt $P \in \mathscr{P}$ and a set of completions $\mathcal{C}_{q}$ for each query.
\begin{equation}
    \mathcal{F}_{\mathcal{D}}^{\text{pairwise}}(q, y, \mathcal{C}_{q}) = \operatorname{WinRate}(\{\operatorname{Compare}(q,y,c)\vsep c\in \mathcal{C}_{q}\}).
\end{equation}
In practice, we combine the output comparison with comparing the output's respective prompts.
These comparisons are then used as optimization signals in the \texttt{Feedback} operator.

\section{Optimization Framework}
Although our first implementation attempt utilized an evolutionary algorithm, we will use a basic population-based hill-climber algorithm.
This design decision has several reasons.
\begin{enumerate}
    \item Most PO research uses a hill-climber architecture.
    \item EAs suffer from slow convergence compared to state-of-the-art hill-climber PO\cite{xiang2025selfsupervisedpromptoptimization}.
    \item PO is complex as it is, and more complicated architectures only introduce more hyperparameters.
\end{enumerate}


\begin{algorithm}
    \caption{Prompt Optimization Hill-Climber}
    \label{alg:promptoptimloop}
    \KwIn{Dataset $\mathcal{D}$, Population size $S$, Iteration count $I$, Batch size $B$}
    \KwOut{Optimized Prompts $\mathscr{P}^{\star}$}
    $\mathcal{D}_{\text{train}}, \mathcal{D}_{\text{dev}}, \mathcal{D}_{\text{test}} \gets \operatorname{Split}(\mathcal{D})$ \tcp{Generate training splits}
    $\mathscr{P} \gets \operatorname{InstructionInduction}(\mathcal{D}_{\text{train}})$ \tcp{Induce initial prompts}
    $i \gets 0$ \tcp{Initialize iteration count}
    $\mathcal{C} \gets \{\}$ \tcp{Initialize solutions} 
    $\mathcal{E} \gets \{\}$ \tcp{Initialize scores}
    $\mathcal{A} \gets \mathscr{P}$ \tcp{All prompts}
    \While{$i<I$}{
        $Q, G \gets \operatorname{RandomSample}(\mathcal{D}_{\text{dev}}, B)$ \\
        $\mathcal{C} \gets \{\mathcal{C}_{q}^{\mathscr{P}}\vsep q \in Q\}$ \\
        $\mathcal{E} \gets \operatorname{Evaluate}(\mathcal{C}, G)$ \\
        $\mathscr{P} \gets \operatorname{Selection}(\mathscr{P}, \mathcal{E})$ \tcp{Pruning} 
        $\mathscr{P} \gets \operatorname{Expand}(\mathscr{P}, \mathcal{C}, \mathcal{E}, \mathcal{D}_{\text{train}})$ \\
        $\mathcal{A} \gets \mathcal{A} \cup \mathscr{P}$ \\
    }
    %$Q_{\text{test}}, G_{\text{test}} \gets D_{\text{test}}$\\
    %$\mathcal{C}_{\text{test}} \gets \{\mathcal{C}_{q}^{\mathcal{A}}\vsep q \in Q_{\text{test}}\}$\\
    %$\mathcal{E}_{\text{test}} \gets \operatorname{Evaluate}(\mathcal{C}, G_{\text{test}})$\\
    $P^{\star} \gets \underset{P\in\mathcal{A}}{\operatorname{argmax}}(\mathcal{E}_{\mathcal{D}_{\text{test}}}(P))$\\
    \Return{$P^{\star}$}
\end{algorithm}

In Algorithm \ref{alg:promptoptimloop} we iterate on the general algorithm \ref{alg:genoptimloop}. 
We will discuss the design of functions used in \ref{alg:promptoptimloop} in following sections.

\begin{itemize}
    \item \textbf{Expand}: The $\operatorname{Expand}$ function can be filled with various expansion operators, of which $\operatorname{InstructionInduction}$
    is a special case. 
    \item \textbf{Evaluate}: Evaluating and identifying the most promising prompts is handled by the $\operatorname{Evaluate}$ operator, which uses task-specific automatic evaluation or LLM-feedback.
    \item \textbf{Selection}: The $\operatorname{Selection}$ operator prunes the population and should maintain only the most promising and diverse prompts for the next expansion.
\end{itemize}

\subsection{Expansion Operator Design}
Expansion operators' job is extending the optimization population with new prompts. Remember notation from \ref{eq:metaprompting}:
\begin{equation*}
    P = \mathscr{M}_{\text{optim}}(M\vsep \mathcal{R}).
\end{equation*}
Notice the use of $\mathscr{M}_{\text{optim}}$, which utilizes non-zero sampling temperature $\tau > 0$ to encourage output diversity. 
Evidently the prompt generation task can be separated into two independent problems: 1. crafting the optimal \textit{Meta-prompt} $M$ 
and 2. designing a data retrieval function $\mathcal{R} = \mathcal{R}(\mathscr{P}, \mathcal{C}, \mathcal{D}, \mathcal{E})$.
The operators' design should address the following challenges:
\begin{enumerate}
    \item \textbf{Loss of generality}: When using task samples $(q, g) \in \mathcal{D}$, the model $\mathscr{M}_{\text{optim}}$ might focus on a single query $q$ and thus fail to generate general instructions.
    \item \textbf{Loss of diversity}: Even for  $\mathscr{M}_{\text{optim}}$ with $\tau>0$, the resulting prompts can be very similar and fail to explore the prompt space $\mathcal{I}$. 
    This ties into a broader exploration vs. exploitation balance issue.
    \item \textbf{Lack of optimization signal}: Research\cite{he2024crispomultiaspectcritiquesuggestionguidedautomatic}\cite{xiang2025selfsupervisedpromptoptimization} suggests that $\mathscr{M}_{\text{optim}}$ 
    can make use of feedback on prompts' outputs and that these textual signals are more effective than numerical scores.
    \item \textbf{Out of distribution \textit{Meta-prompt}}: Prompt engineering is a novel research area and does not have a substantial support in the LLM's training corpus.
    The \textit{Meta-prompt} $M$ thus has to be carefully constructed to help the model output relevant prompts.
\end{enumerate}

We now discuss the design of each prompt generation operator and display their signatures and \textit{meta-prompts}. 
Note that all operators are ultimately used in a CoT context, where a \texttt{reasoning} field is prepended to each signature's outputs.
\subsubsection{Lamarckian}
Instruction Induction\cite{honovich2022instructioninductionexamplesnatural} is used by many PO methods and often referred to as \texttt{Lamarckian Mutation}. 
We will adopt this terminology from now on and design our \texttt{Lamarckian} operator. 
Design of its meta-prompt, shown in figure \ref{box:lamarcksig}, takes into account the design challenges mentioned earlier by 1. warning the LLM to be general and not to focus on a single example, 
2. clearly states the problem using a directive and formatting specifications. 

The problem with diversity still persists, and we consider two approaches to solving it. 
We can increase the model's creativity by increasing its sampling temperature. Another approach is to use
some kind of \textit{seed}, for example a \textit{persona}. We experiment with using personas from PersonaHub\cite{ge2024scalingsyntheticdatacreation}.
Authors of this paper argued that seeding generation with the persona helps with creating novel synthetic data. 

For the data retrieval part, \texttt{Lamarckian} utilizes only examples of the datasets. 
We randomly sample $N$ examples from a separate training split. So
\begin{equation}
    \mathcal{R}_{\text{L}}(\mathcal{D}) = \operatorname{RandomSample}(\mathcal{D}_{\text{train}}, N)
\end{equation}
\begin{figurebox}{Lamarckian Signature. Formatting specifications trimmed.}{box:lamarcksig}
    \hlbox{ctuorange}{Task examples: \texttt{str} (Samples from a problem class) \\
    Persona (Optional): \texttt{str} (Assume this persona when writing the prompt)} 
    \hlbox{brown}{Prompt proposal: \texttt{str} (Instructions for solving the problem)} 
    \hlbox{ctublue}{Craft a \textbf{general} developer prompt to help an LLM with solving a class of problems.\\
    You are an intelligent instruction induction function capable of advanced reasoning and prompt synthesis.\\
    Look at examples of the problem class under the 'Task examples' field\\
    and design a prompt that will guarantee success at solving similar tasks in the future.\\
    Make sure your instructions are \textbf{TRULY GENERAL} and apply to all given samples \textbf{simultaneously}.
}
\end{figurebox}
\newpage
\subsubsection{Iterative}
The \texttt{Iterative} operator is one of the most common and simplest operators. 
It uses a sequence of prompts and their scores in ascending order.
The hope is for the LLM to deduce the optimization direction by looking at the differences in the prompts and incite it to continue the pattern.

Although some research\cite{yang2024largelanguagemodelsoptimizers} only uses the top prompts, we opt for a roulette selection method
and sort to prompts by score in ascending order. The number $N$ is a hyperparameter dictating how many prompts to sample.
We define the retrieval function as
\begin{equation}
   \mathcal{R}_{\text{I}}(\mathscr{P}, \mathcal{E}) = \operatorname{SortByScore}(\operatorname{RouletteSampling}(\mathscr{P}, \mathcal{E}, N), \mathcal{E})
\end{equation}
Other methods\cite{tang2024unleashingpotentiallargelanguage} additionally augment the \textit{meta-prompt} with task samples, similar to the \texttt{Lamarckian} operator.

In the \textit{meta-prompt}, we instruct the LLM to try to follow the sequence. Also, we specifically say to 'craft a new prompt'
as opposed to 'improve a prompt' to incite more novelty. For formatting, we use the same instruction set as in the \texttt{Lamarckian}.
\begin{figurebox}{Iterative Signature. Formatting specifications trimmed.}{box:itersig}
    \hlbox{ctuorange}{Old prompts: \texttt{list} (List of previous prompts with scores)}
    \hlbox{brown}{Prompt proposal: \texttt{str} (Better prompt)} 
    \hlbox{ctublue}{Craft a new prompt for an LLM.
    
    You are an intelligent pattern continuation function capable of advanced reasoning and prompt synthesis.\\
    You are given a history of past prompts along with their scores.\\
    They are listed in ascending order of fitness.\\
    Follow the sequence and design an improved prompt. \\
}
\end{figurebox}

\subsubsection{Reflective}
Recent PO literature\cite{xiang2025selfsupervisedpromptoptimization} shifts to using LLM outputs as optimization signals
and argues that utilizing only numerical signals is ineffective. To address this, we design an exploitative operator, which
aims to fix faults in the prompt by analyzing its failed attempt at a task sample. 

To achieve this, a more complex \texttt{Signature} is utilized. Its outputs guide the LLM to first critique the original prompt
and then improve it. Instructions are more complete with a step-by-step guide which explains the task clearly.
Note that 1. now we use "improve" wording, 2. we stress to only alter the prompt \textit{slightly}. This is done due to 
frequent observation of the model just creating an entirely different prompt only applicable to the single example task.
For formatting, we use the same instructions as in previous \textit{meta-prompts}.

In $\mathcal{R}$, we want to select the worst possible attempt. This means we optimize "from the bottom up" and try to bootstrap the
worst prompts. The retrieval function is
\begin{equation}
    \mathcal{R}_{\text{R}}(\mathscr{P}, \mathcal{C}, \mathcal{D}, \mathcal{E}) = \operatorname{JoinAttemptWithTask}(\operatorname{FindWorstAttempt}(\mathscr{P}, \mathcal{C}, \mathcal{E}, \mathcal{D})
\end{equation}

\begin{figurebox}{Reflective Signature. Formatting specifications trimmed.}{box:reflexsig}
    \hlbox{ctuorange}{Original prompt: \texttt{str} (Improve this prompt) \\
    Task question: \texttt{str} (Task on which the prompt was used) \\
    Solution: \texttt{str} (What the original prompt produced)}
    \hlbox{brown}{Original prompt critique: \texttt{str} (Faults in the original prompt) \\
    Prompt proposal: \texttt{str} (Improved prompt)} 

    \hlbox{ctublue}{Improve a prompt for an LLM.
    
    You are an intelligent reflection function capable of advanced reasoning and prompt synthesis.\\
    Follow these steps to craft a better prompt:\\
    - Analyze the original prompt and its suboptimal performance on a task sample.\\
    - Find failure points in the solution and cross-reference to identify weaknesses in the prompt.\\
    - Think of a critique that captures your findings\\
    - Apply your critique to \textit{slightly} alter the original prompt to improve it.\\
    Your improved prompt should still be \textbf{widely applicable and generic}.}
\end{figurebox}


\subsubsection{Feedback}
As we mentioned earlier, the \texttt{Feedback} operator is suitable for use in self-supervised settings.
It leverages reasoning traces from pairwise LLM-based comparisons, discussed in \ref{sec:ssometrics}. 
Let $\mathcal{E}_{\text{comp}}$ hold textual comparisons of each prompt and their attempts and
$P_{\text{base}} = \operatorname{RandomSample}(\mathscr{P})$. Then
\begin{equation}
    \mathcal{R}_{\text{F}}(\mathscr{P}, \mathcal{E}_{\text{comp}}) = \{P_{\text{base}}, \operatorname{GetComparisons}(P_{\text{base}}, \mathcal{E}_{\text{comp}})\}
\end{equation}
In the \textit{Meta-prompt}, we frame the task as critique synthesis and use "improve" wording to guide the LLM to start from the base prompt.
We also explain that each comparison has a different verdict and the base prompt might not always be the winner. For the formatting guide, we use the same instructions
as in the previous operators.

For large populations or tasks producing long prompts, we might run into issues with LLM context window length. 
However for our purpose, modern LLMs provide more than sufficient context limits. 
\begin{figurebox}{Feedback Signature. Formatting specifications trimmed.}{box:feedbacksig}
    \hlbox{ctuorange}{
        Base prompt: \texttt{str} (Improve this prompt) \\
        Comparisons: \texttt{str} (Base prompt compared to others)
    }
    \hlbox{brown}{Prompt proposal: \texttt{str} (Improved prompt)} 
    \hlbox{ctublue}{Improve a prompt for an LLM.

    You are an intelligent critique synthesis function capable of advanced reasoning. \\
    You are given a base prompt and a list of comparisons between the base prompt and other prompts.\\
    Some other prompts are better than the base prompt, some are worse.\\
    Your task is to analyze the comparisons and synthesize a new prompt that incorporates the feedback.}
\end{figurebox}

\subsubsection{Paraphrase}
To serve as another baseline for other operators, we implement a simple \texttt{Paraphrase} operator.
This operator performs random search in the prompt space by changing the wording and structure of a prompt.
The prompt is selected via the retrieval function
\begin{equation}
    \mathcal{R}_{\text{P}}(\mathscr{P}, \mathcal{E}) = \operatorname{RouletteSampling}(\mathscr{P}, \mathcal{E}).
\end{equation}
This method uses no optimization signal or improvement instructions and relies on pure chance of finding a more potent prompt.


\begin{figurebox}{Paraphrase Signature. Formatting specifications trimmed.}{box:parasig}
    \hlbox{ctuorange}{Original prompt: \texttt{str} (Prompt to paraphrase)}
    \hlbox{brown}{Prompt proposal: \texttt{str} (Paraphrased prompt)} 
    \hlbox{ctublue}{Paraphrase a prompt for an LLM.
                    
    You are an intelligent paraphrasing function capable of advanced reasoning and prompt synthesis.\\
    You are given a prompt and your task is to paraphrase it. \\
    Use synonyms and change the structure of the prompt but keep it semantically equivalent.}
\end{figurebox}
In figure \ref{box:formatting} we show the formatting specification part, which is identical for all prompt generation \textit{meta-prompts}.

\begin{figurebox}{Formatting specifications, which are the same for all operators}{box:formatting}
    Use Markdown formatting in your final answer to indicate bullet points and whatever else necessary.\\
    As a placeholder for the task question, '<INSERT TASK QUESTION HERE>' should be used exactly ONCE.\\
    In the final answer, do not include a title or any additional data, just the prompt.
\end{figurebox}

\subsection{Selection Operator}
At the start of each optimization step, we select $n_{\text{continue}}$ prompts to continue in the process and purge the rest. 
To achieve better prompt diversity, a method based on edit distance is used. 

This method, outlined in Algorithm \ref{alg:duplicpurge},
removes the closest prompt for each prompt, starting from the best prompts. This ensures that performant prompts are kept and their worse-performing duplicates are deleted.
We opt to use edit distance instead of semantic similarity, like BERT embeddings.

\begin{algorithm}
    \caption{Purge Duplicates}
    \label{alg:duplicpurge}
    \KwIn{Population $\mathscr{P}$, Pruning factor $f_{\text{prune}}$}
    \KwOut{Pruned population $\mathscr{P}_{\text{pruned}}$}
    $\mathscr{P}_{\text{sorted}} \gets \operatorname{SortByScore}(\mathscr{P}, \mathcal{E})$ \\
    $n_{\text{continue}} \gets \vert\mathscr{P}\vert(1-f_{\text{prune}})$
    $i \gets 0$
    \While{$i<n_{\text{continue}}$} {
        $P_{\text{select}} \gets \operatorname{GetFirst}(\mathscr{P}_{\text{sorted}})$ \\ 
        $P_{\text{purge}} \gets \underset{P\in\mathscr{P}\mid P \neq P_{\text{select}}}{\operatorname{argmax}} \operatorname{LevenshteinRatio}(P, P_{\text{select}})$ \\
        $\operatorname{Remove}(\mathscr{P}, P_{\text{purge}})$
    }
    $\mathscr{P}_{\text{pruned}} \gets \mathscr{P}$\\
    \Return{$\mathscr{P}$}
\end{algorithm}
