\section{Inference framework}
Taking inspiration from DSPy\cite{khattab2023dspycompilingdeclarativelanguage}, we first implement a simple LLM-calling framework 
capable of invoking several selected inference strategies. Motivations for this are twofold:
\begin{enumerate}
    \item DSPy is a young and ambitious project aiming at simplifying LLM pipeline design and optimization. 
    As we focus on single-stage prompt program optimization, this capability is not useful for our work. 
    Furthermore, due to the framework's infancy, it lacks proper documentation and sometimes exhibits unexpected behavior.
    \item Implementing the prompting techniques discussed in \ref{sec:inference} provides further insight into their workings and performance.
\end{enumerate}

\subsection{Structured generation}
Following current research trends\cite{zhang2025metapromptingaisystems}, we build our inference framework around a structured JSON template,
or a \texttt{Signature}. The \texttt{Signature} structure consists of input and output fields and additional instructions. 
These fields are populated by a \texttt{Field} data structure.
Of particular interest are the output fields, which hold the output name, desired type and optional description. 

When employing good naming practices the model can often deduce the task only by looking at output names and types.
Consider the following \texttt{Signature}:
\begin{promptbox}{Simple Signature}
    \hlspan{ctuorange}{Word: \texttt{str}} $\rightarrow$ \hlspan{ctuorange}{Antonymum: \texttt{str}}
\end{promptbox}
\newpage
For more complex tasks, filling the output descriptions or even adding explicit instructions is necessary.
In \ref{box:complexsig} notice that it is possible to specify multiple inputs and outputs, which are then generated in the order given.
\begin{promptbox}[label={box:complexsig}]{Complex Signature}
    \hlspan{ctuorange}{Text: \texttt{str}, Grading guide: \texttt{str}} 
    $\rightarrow$ 
    \hlspan{ctuorange}{Evaluation: \texttt{str}, Grade: \texttt{int}} \hfill
    \\ \\
    Instructions:
    \hlbox{ctublue}{Grade the text.\\ You are an expert text evaluator. \\
    Use the grading guide to evaluate the test and give a final grade. 
    Use formal language and markdown formatting in the evaluation\\ and output a 1-10 integer for the grade.}
\end{promptbox}

Sufficiently large instruction-tuned LLMs are usually good at reliably producing JSON output.
For smaller models or more complex output structures, it might be necessary to use some form of constrained generation as discussed in \ref{sec:inference}.
A JSON schema could be constructed automatically from the \texttt{Signature} and passed into a parser-based sampler.
However this is not necessary for our use-case.
\subsection{Predict method}
To facilitate \texttt{Signature}-powered generation, we implement a \texttt{predict} method that involves
\begin{enumerate}
    \item Prepending a developer prompt to the messages
    \item Parsing of \texttt{Signature} outputs
    \item Repeated generation in case of parsing failure.
\end{enumerate}
\newpage

\begin{promptbox}[label={box:predictdev}]{Predict method developer prompt}
    \hlbox{ctuorange}{You are an intelligent function that returns structured JSON outputs matching a given schema.
    }
    
    \hlbox{ctulightblue}{
    You will receive a JSON object containing: \\
        - `inputs`: a dictionary of named inputs \\
        - `outputs`: a dictionary specifying the expected output fields with their types and descriptions\\
        - `instructions`: a task or question to answer (optional)\\

    Your job is to:\\
        1. Understand the task from `instructions` or infer it from `inputs` and `outputs`\\
        2. Use the `inputs` to compute or generate the answer\\
        3. Respond **only** with keys from the `outputs` dictionary and values matching the described types
    }
    \hlbox{ctublue}{Only return a flat JSON object like:\\
    \{\\
    "field1": <value matching type and description>,\\
    "field2": <...>\\
    \}}
    
    \hlbox{ctublue!50!black}{Do not add metadata, explanations, or wrap outputs in additional structures.\\
    Do not include type names or field descriptions in the output.\\
    Your output must be strictly valid JSON and fill **all** requested output fields.}
\end{promptbox}
The developer prompt has to clearly explain to the LLM how to work with the JSON-based \texttt{Signature}.
In \ref{box:predictdev} notice the sections of the prompt following principles outline in \ref{sec:preng}.
First, the directive states the task, then a further context is added about the \texttt{Signature} data structure and the task.
Next, notice the example showing the proper output, and finally few more clarifying instructions about the output format.
In experiments, this prompt is successful in incentivizing parseable outputs adhering to the specifications.

Parsing the output presents some challenges as the LLM sometimes wraps the JSON output into a markdown code block
or uses inconsistent escape sequences. We implement a simple parses based on regular expressions that is able to parse 
a majority of outputs. In case of model failure, such as getting stuck in a generation loop, we add a repeated generation
feature.
\newpage
\subsection{Inference techniques implementation}
Leveraging the \texttt{predict} method and the modular \texttt{Signature}-based interface, we implement a suite of inference-time prompting techniques. 
Each technique is realized through systematic modifications of the \texttt{Signature} fields, changing the developer prompt and the chaining of multiple generation steps 
and function calls. This design allows for composability and reuse while preserving transparency.
We implement the following methods.
\begin{enumerate}
    \item \textbf{Chain-of-thought}\cite{NEURIPS2022_8bb0d291}: Prepends a reasoning field to the \texttt{Signature} outputs which forms a scratchpad for the LLM.
    \item \textbf{Chain-of-thought with Self-consistency}\cite{wang2023selfconsistencyimproveschainthought}: Multiple CoT generations with majority-voting.
    \item \textbf{ReAct}\cite{yao2023reactsynergizingreasoningacting}: Adding tools allows the LLM to interleave thoughts and action steps.
    \item \textbf{Program-of-thought}\cite{chen2023programthoughtspromptingdisentangling}: Two-stage CoT with Python-code execution
    \item \textbf{Reflexion}\cite{shinn2023reflexionlanguageagentsverbal}: After an initial generation, the model is prompted to self-critique and revise its output.
    \item \textbf{Tree of Thoughts}\cite{yao2023treethoughtsdeliberateproblem}: The problem is first decomposed and each step is expanded, forming a thought tree, which is then traversed with BFS or DFS.
\end{enumerate}

\section{Datasets}
In this section, we discuss choosing datasets for testing our method and comparing various prompt optimization approaches. 
While searching available datasets, we focus on the following criteria:
\begin{enumerate}
    \item \textbf{Output complexity}: We focus on more complex outputs. Specifically, datasets with multiple-choice or Yes/No answers are omitted. 
    This disqualifies commonly used datasets as MMLU or BigBenchHard.
    \item \textbf{Contamination}: Recently, researchers have expressed concern\cite{white2025livebenchchallengingcontaminationlimitedllm} 
    whether benchmarks are reliable evaluations of models as they might appear in their training data. We omit most common datasets, such as GSM8k\cite{cobbe2021gsm8k}, which has been shown to have enflated scores for some models\cite{testing_language_models_on_a_held_out_high_school_national_finals_exam}.
    \item \textbf{Output verification}: We prefer to use simple automatic verification rather that using LLM-as-a-judge, which has been shown to be 
    biased in some circumstances\cite{ye2024justiceprejudicequantifyingbiases}, and human feedback, which defeats the purpose of automatic prompt optimization.
    \item \textbf{Difficulty}: We omit tasks where models already have near-perfect score. 
    \item \textbf{Benefit from non-trivial instruction}: We focus on tasks where helpful hints and step-by-step tutorial-like instructions would be helpful.
\end{enumerate}
We now list the datasets we will use for evaluation and explain why they were chosen.
\subsection{Livebench}
The Livebench\cite{white2025livebenchchallengingcontaminationlimitedllm} dataset is very recent and has been created with the issue of data contamination in mind.
It also addresses the issues of LLM-as-a-judge verification and all its categories can be verified automatically. It is also very challenging, with top models achieving $65\%$ accuracy\cite{white2025livebenchchallengingcontaminationlimitedllm}.

Out of the tasks available in Livebench, we select the \texttt{Connections} task from the \texttt{Language} subset. 
This task consists of sorting given words into non-trivial groups of four based on semantics, phonetics and other features. 
An ideal prompt would attempt to list multiple possible aspects based on which the words can be sorted and also include a helpful example.

\subsection{Code Contests}
Programming puzzles are a difficult and easily verifiable task. Although \texttt{CodeContests}\cite{li2022competition} is an older dataset, 
we hope it poses lesser contamination risks than datasets with simpler outputs. With LLM-powered coding assistants on the rise, we
feel this is a relevant application area for out method. 

\subsection{Sequences}
We design a small but challenging dataset consisting of predicting the next number in an integer sequence.
Each sequence is created according to a formula with randomly selected coefficients. The formulas fall into several categories, for example
\begin{itemize}
    \item \textbf{Linear with modulo}: $s(i) = \operatorname{mod}_{q}(a_{1} i + b_{1})$
    \item \textbf{Sum}: $s(i) = \sum_{j=0}^{i-1}a_{1} j + b_{1}$
    \item \textbf{Alternating}: $s(i) = a_{1}i + a_{2}i(-1)^{i}$.
\end{itemize}
This tests the model's ability to 1. detect and understand patterns and 2. systematically perform simple arithmetic. 
In practice, we will optimize just for a single sequence category and observe, whether the optimizer evolves a prompt with a tutorial for the specific sequence category.
Experiments revealed that the \texttt{Alternating} class of sequences has a good difficulty balance and we will use it for evaluation.



\section{Evaluation Metrics}
The evaluation metric defines the optimization goal and thus forms its central component. 
Most evaluation metrics are task-specific and divisible into two categories based on whether they are used in a supervised or self-supervised context.
\subsection{Metrics for Supervised Optimization}
Supervised optimization is supported by gold labels and its underlying metrics all perform comparisons between the results and the gold labels.
These include classification metrics, like accuracy or Hamming Loss, regression metrics, like Mean Squared Error, and many others.

All three main benchmarks that we will use (\texttt{Connections}, \texttt{CodeContests}, \texttt{Sequences}) 
fall into this category. For each benchmark we use a simple accuracy metric. Given a dataset $\mathcal{D}$ and questions $q$ and gold labels $g$, $(q,g) \in \mathcal{D}$:
\begin{enumerate}
    \item \textbf{Connections}:  $\mathcal{F}_{\mathcal{D}_{\text{Conn}}}(q, g) = \operatorname{Overlap}(\operatorname{Groups}(q), \operatorname{Groups}(g))$
    \item \textbf{CodeContests}: $\mathcal{F}_{\mathcal{D}_{\text{Code}}} = \operatorname{FinishesExecution}(q) + \operatorname{PassesAllCases}(q, g)$
    \item \textbf{Sequences}: $\mathcal{F}_{\mathcal{D}_{\text{Seq}}} = \operatorname{Equals}(q, g)$
\end{enumerate} 

\subsection{Metrics for Self-Supervised Optimization}\label{sec:ssometrics}
In self-supervised contexts, metrics are usually based on reward models pretrained on human preference or environment data.
To allow our method to be applied to gold label-free problems, we turn to LLM-based direct pairwise comparisons.
Given a dataset $\mathcal{D}$ with questions $q$, output $y$ produced by prompt $P \in \mathcal{P}$ and completions $\mathcal{C}$
\begin{equation}
    \mathcal{F}_{\mathcal{D}}^{\text{pairwise}}(q, y, \mathcal{C}) = \operatorname{WinRate}(\{\operatorname{Compare}(q,y,c)\vsep c\in\operatorname{Attempts}(q,\mathcal{C})\}).
\end{equation}
In practice, we combine the output comparison with comparing the output's respective prompts.
These comparisons are then used as optimization signals in the \texttt{Feedback} operator.

\section{Optimization Framework}
Although our first implementation attempt utilized an evolutionary algorithm, we will use a basic population-based hill-climber algorithm.
This design decision has several reasons.
\begin{enumerate}
    \item Most PO research uses a hill-climber architecture.
    \item EAs suffer from slow convergence compared to state-of-the-art hill-climber PO\cite{xiang2025selfsupervisedpromptoptimization}.
    \item PO is complex as it is and more complicated architectures only introduce more hyperparameters.
\end{enumerate}


\begin{algorithm}
    \caption{Prompt Optimization Hill-Climber}
    \label{alg:promptoptimloop}
    \KwIn{Dataset $\mathcal{D}$, Population size $S$, Iteration count $I$, Batch size $B$}
    \KwOut{Optimized Prompts $\mathcal{P}^{\star}$}
    $\mathcal{D}_{\text{train}}, \mathcal{D}_{\text{dev}}, \mathcal{D}_{\text{test}} \gets \operatorname{Split}(\mathcal{D})$ \tcp{Generate training splits}
    $\mathcal{P} \gets \operatorname{InstructionInduction}(\mathcal{D}_{\text{train}})$ \tcp{Induce initial prompts}
    $i \gets 0$ \tcp{Initialize iteration count}
    $\mathcal{C} \gets \{\}$ \tcp{Initialize solutions} 
    $\mathcal{E} \gets \{\}$ \tcp{Initialize scores}
    $\mathcal{A} \gets \mathcal{P}$ \tcp{All prompts}
    \While{$i<I$}{
        $Q, G \gets \operatorname{RandomSample}(\mathcal{D}_{\text{dev}}, B)$ \\
        $\mathcal{C} \gets \{\mathcal{C}_{q}^{\mathcal{P}}\vsep q \in Q\}$ \\
        $\mathcal{E} \gets \operatorname{Evaluate}(\mathcal{C}, G)$ \\
        $\mathcal{P} \gets \operatorname{Selection}(\mathcal{P}, \mathcal{E})$ \tcp{Pruning} 
        $\mathcal{P} \gets \operatorname{Expand}(\mathcal{P}, \mathcal{C}, \mathcal{E}, \mathcal{D}_{\text{train}})$ \\
        $\mathcal{A} \gets \mathcal{A} \cup \mathcal{P}$ \\
    }
    $Q_{\text{test}}, G_{\text{test}} \gets D_{\text{test}}$\\
    $\mathcal{C}_{\text{test}} \gets \{\mathcal{C}_{q}^{\mathcal{A}}\vsep q \in Q_{\text{test}}\}$\\
    $\mathcal{E}_{\text{test}} \gets \operatorname{Evaluate}(\mathcal{C}, G_{\text{test}})$\\
    $P^{\star} \gets \underset{P\in\mathcal{A}}{\operatorname{argmax}}(\mathcal{E}_{\text{test}})$\\
    \Return{$P^{\star}$}
\end{algorithm}
In \ref{alg:promptoptimloop} we iterate on the general algorithm \ref{alg:genoptimloop}. 
We will discuss the design of functions used in \ref{alg:promptoptimloop} in following sections.
\begin{itemize}
    \item \textbf{Expand}: The $\operatorname{Expand}$ function can be with many different expansion operators, of which $\operatorname{InstructionInduction}$
    is a special case. 
    \item \textbf{Evaluate}: Evaluating and identifying the most promising prompts is handled by the $\operatorname{Evaluate}$ operator, which uses task-specific automatic evaluation or LLM-feedback.
    \item \textbf{Selection}: The $\operatorname{Selection}$ operator prunes the population and should maintain only the most promising and diverse prompts for the next expansion.
\end{itemize}

\subsection{Expansion Operator Design}
Expansion operators' job is extending the optimization population with new prompts. Remember notation from \ref{eq:metaprompting}:
\begin{equation*}
    P = \mathscr{M}_{\text{optim}}(M\vsep \mathcal{R}).
\end{equation*}
Notice the use of $\mathscr{M}_{\text{optim}}$, which utilizes non-zero sampling temperature. Evidently the prompt generation task can be separated into two independent problems: 1. crafting the optimal \textit{Meta-prompt} $M$ 
and 2. designing a data retrieval function $\mathcal{R} = \mathcal{R}(\mathcal{P}, \mathcal{C}, \mathcal{D}, \mathcal{E})$.
The operators' design should address the following challenges:
\begin{enumerate}
    \item \textbf{Loss of generality}: When using task samples $(q, g) \in \mathcal{D}$, the model $\mathcal{M}_{\text{optim}}$ might focus on some $t$ and thus fail to generate general instructions.
    \item \textbf{Loss of diversity}: Even for  $\mathscr{M}_{\text{optim}}$ with $t>0$, the resulting prompts can be very similar and fail to explore the prompt space $\mathcal{I}$. 
    This ties into a broader exploration vs. exploitation balance issue.
    \item \textbf{Lack of optimization signal}: Research\cite{he2024crispomultiaspectcritiquesuggestionguidedautomatic}\cite{xiang2025selfsupervisedpromptoptimization} suggests that $\mathcal{M}_{\text{optim}}$ 
    can make use of feedback on prompts' outputs and that these textual signals are more effective than numerical scores.
    \item \textbf{Out of distribution \textit{Meta-prompt}}: Prompt engineering is a novel research area and does not have a substantial support in the LLM's training corpus.
    The \textit{Meta-prompt} $M$ thus has to be carefully constructed to help the model output relevant prompts.
\end{enumerate}

We now discuss the design of each prompt generation operator and display their signatures and \textit{meta-prompts}. 
Note that all operators are ultimately used in a CoT context, where a \texttt{reasoning} field is prepended to each signature's outputs.
\subsubsection{Lamarckian}
Instruction Induction\cite{honovich2022instructioninductionexamplesnatural} is used by many PO methods and often refered to as \texttt{Lamarckian Mutation}. 
We will adopt this terminology from now on and design our \texttt{Lamarckian} operator. 
Design of its meta-prompt takes into account the design challenges mentioned earlier by 1. warning the LLM to be general and not to focus on a single example, 
2. clearly states the problem using a directive and formatting specifications. 

The problem with diversity still persists and we consider two approaches to solving it. 
We can increase the model's creativity by increasing its sampling temperature. Another approach is to use
some kind of \textit{seed}, for example a \textit{persona}. We experiment with using personas from PersonaHub\cite{ge2024scalingsyntheticdatacreation}.
Authors of this paper argued that seeding generation with the persona helps with creating novel synthetic data. 

For the data retrieval part, \texttt{Lamarckian} utilizes only examples of the datasets. 
We randomly sample $N$ examples from a separate training split. So
\begin{equation}
    \mathcal{R}_{\text{L}}(\mathcal{D}) = \operatorname{RandomSample}(\mathcal{D}_{\text{train}}, N)
\end{equation}


\subsubsection{Iterative}
The \texttt{Iterative} operator is one of the most common and simplest operators. 
It uses a sequence of prompts and their scores often in an ascending order.
The hope is for the LLM to deduce the optimization direction by looking at the differences in the prompts and incite it to continue the pattern.

Although some research\cite{yang2024largelanguagemodelsoptimizers} only uses the top prompts, we opt for a roulette selection method
and sort to prompts by score in an ascending order. The number $N$ is a hyperparameter dictating how many prompts to sample.
We define the retrieval function as
\begin{equation}
   \mathcal{R}_{\text{I}}(\mathcal{P}, \mathcal{E}) = \operatorname{SortByScore}(\operatorname{RouletteSampling}(\mathcal{P}, \mathcal{E}, N), \mathcal{E})
\end{equation}
Other methods\cite{tang2024unleashingpotentiallargelanguage} also include task examples, like in the \texttt{Lamarckian}.

In the metaprompt, we instruct the LLM to try to follow the sequence. Also, we specifically say to 'craft a new prompt'
as opposed to 'improve a prompt' to incite more novelty. For formatting, we use the same instruction set as in the \texttt{Lamarckian}.

\subsubsection{Reflective}
Recent PO literature\cite{xiang2025selfsupervisedpromptoptimization} shifts to using LLM outputs as optimization signals
and argues that utilizing only numerical signals is ineffective. To address this, we design an exploitative operator, which
aims to fix faults in the prompt by analyzing its failed attempt at a task sample. 

To achieve this, a more complex \texttt{Signature} is utilized. Its outputs guide the LLM to first critique the original prompt
and then improve it. Instructions are more complete with a step-by-step guide which explains the task clearly.
Note that 1. now we use "improve" wording, 2. we stress to only alter the prompt \textit{slightly}. This is done due to 
frequent observation of the model just creating an entirely different prompt only applicable to the example task.
For formatting, we use the same instructions as in previous \textit{meta-prompts}.

In $\mathcal{R}$, we want to select the worst possible attempt. This means we optimize "from the bottom up" and try to bootstrap the
worst prompts. The retrieval function is
\begin{equation}
    \mathcal{R}_{\text{R}}(\mathcal{P}, \mathcal{C}, \mathcal{D}, \mathcal{E}) = \operatorname{JoinAttemptWithTask}(\operatorname{FindWorstAttempt}(\mathcal{P}, \mathcal{C}, \mathcal{E}, \mathcal{D})
\end{equation}


\subsubsection{Feedback}
As we mentioned earlier, the \texttt{Feedback} operator is suitable for use in self-supervised settings.
It leverages reasoning traces from pairwise LLM-based comparisons, discussed in \ref{sec:ssometrics}. 
Let $\mathcal{E}_{\text{comp}}$ hold textual comparisons of each prompts and their attempts and
$P_{\text{base}} = \operatorname{RandomSample}(\mathcal{P})$. Then
\begin{equation}
    \mathcal{R}_{\text{F}}(\mathcal{P}, \mathcal{E}_{\text{comp}}) = \{P_{\text{base}}, \operatorname{GetComparisons}(P_{\text{base}}, \mathcal{E}_{\text{comp}})\}
\end{equation}
In the \textit{Meta-prompt}, we frame the task as critique synthesis and use "improve" wording to guide the LLM to start from the base prompt.
We also explain that each comparisons has a different verdict and the base prompt might not always be the winner. For the formatting guide, we use the same instructions
as in the previous operators.

For large populations or tasks producing long prompts, we might run into issues with LLM context window length. 
However for our purpose, modern LLMs provide more than sufficient context limits. 

\subsubsection{Paraphrase}
To serve as another baseline for other operators, we implement a simple \texttt{Paraphrase} operator.
This operator performs random search in the prompt space by changing the wording and structure of a prompt.
The prompt is selected via the retrieval function
\begin{equation}
    \mathcal{R}_{\text{P}}(\mathcal{P}, \mathcal{E}) = \operatorname{RouletteSampling}(\mathcal{P}, \mathcal{E}).
\end{equation}
This method uses no optimization signal or improvement instructions and relies on pure chance of finding a more potent prompt.


\subsection{Selection Operator}
At the start of each optimization step, we select $n_{\text{continue}}$ prompts to continue in the process and purge the rest. 
To achieve better prompt diversity, a method based on edit distance is used. This method, outlined in Algorithm \ref{alg:duplicpurge},
removes the closest prompt for each prompt, starting from the best. This ensures that performant prompts are kept and their worse-performing duplicates are deleted.
We opt to use edit distance instead of semantic similarity, like BERT embeddings.

\begin{algorithm}
    \caption{Purge Duplicates}
    \label{alg:duplicpurge}
    \KwIn{Population $\mathcal{P}$, Pruning factor $f_{\text{prune}}$}
    \KwOut{Pruned population $\mathcal{P}_{\text{pruned}}$}
    $\mathcal{P}_{\text{sorted}} \gets \operatorname{SortByScore}(\mathcal{P}, \mathcal{E})$ \\
    $n_{\text{continue}} \gets \vert\mathcal{P}\vert(1-f_{\text{prune}})$
    $i \gets 0$
    \While{$i<n_{\text{continue}}$} {
        $P_{\text{select}} \gets \operatorname{GetFirst}(\mathcal{P}_{\text{sorted}})$ \\ 
        $P_{\text{purge}} \gets \underset{P\in\mathcal{P}\mid P \neq P_{\text{select}}}{\operatorname{argmax}} \operatorname{LevenshteinRatio}(P, P_{\text{select}})$ \\
        $\operatorname{Remove}(\mathcal{P}, P_{\text{purge}})$
    }
    $\mathcal{P}_{\text{pruned}} \gets \mathcal{P}$\\
    \Return{$\mathcal{P}$}
\end{algorithm}
