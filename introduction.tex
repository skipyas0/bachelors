\section{Background}
In recent years, Large Language Models (LLMs) have permeated the Natural Language Processing research landscape as well as into the general public. 
Already achieving human-like performance at a wide variety of tasks\cite{bubeck2023sparksartificialgeneralintelligence}, 
they are bound by scaling laws\cite{kaplan2020scalinglawsneurallanguage} which predict performance gained with adding compute, fueling
massive investments into computation capacity by industry players. 

With costs of training new state-of-the-art foundational LLMs rising rapidly, research has turned to inference-time scaling\cite{welleck2024decodingmetagenerationinferencetimealgorithms}, 
based on post-training\cite{openai2024openaio1card}\cite{deepseekai2025deepseekr1incentivizingreasoningcapability} utilizing reinforcement learning and supervised fine-tuning, and 
prompting techniques\cite{schulhoff2024promptreportsystematicsurvey}. 

Another research branch gaining substantial attention recently is compile-time scaling\cite{schnabel2024symbolicpromptprogramsearch} represented by prompt optimization\cite{ramnath2025systematicsurveyautomaticprompt}.
Optimization using LLMs\cite{meyerson2024languagemodelcrossovervariation}\cite{liu2024largelanguagemodelsevolutionary} and particularly prompt optimization\cite{yang2024largelanguagemodelsoptimizers}\cite{zhou2023largelanguagemodelshumanlevel}\cite{he2024crispomultiaspectcritiquesuggestionguidedautomatic} 
presents an exciting intersection between deep learning and traditional optimization algorithms, like evolutionary algorithms\cite{guo2024connectinglargelanguagemodels}\cite{cui2024phaseevounifiedincontextprompt}\cite{fernando2023promptbreederselfreferentialselfimprovementprompt} and other metaheuristics\cite{pan2024plumpromptlearningusing}.

