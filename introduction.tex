\section{Background}
In recent years large language models (LLMs) emerged as a revolutionary force not only in the field natural language processing (NLP) but in everyday life of the general population \todo{cite smth}. 
The fundamental way to interact with these models is via text instructions or \textbf{prompts}. These prompts can be standalone or act as templates into which additional information,
like a specific task or context, like from a database search, is inserted. This introduces the problem of \textbf{prompt engineering}, or finding the optimal prompt for a given situation.
Prompt engineering emerges as a largely empirical field, with even experts experienced in the inner workings of LLMs relying on extensive trial-and-error bouts before finding a suitable prompt. 
With even experts struggling, it is significantly harder for non-experts to design an effective prompt. This motivates the recent research into automated prompt engineering. Several research directions have
emerged, such as soft prompt tuning and discrete prompt optimization. Soft prompt tuning is effective but relies on access to inner states of the LLMs with are unavailable when using current state-of-the-art APIs such as
the OpenAI API. Furthermore, prompts resulting from soft prompt tuning are often unreadable. In this work we focus on methods of discrete prompt optimization. This problem is very challenging because of the high dimesionality of the language space in which we
conduct the search and non-differentiability as there are no gradients in the text space.

\section{Objectives}
The goal of this thesis is to compare several prompt optimization methods utilizing LLMs as the optimizing actors. That means, given a prompt $p_t$, where 
$t$ denotes a step in the optimization process, the next prompt can be formulated as $p_{t+1}=\mathfrak{L}_{iter}(p_t)$, where $\mathfrak{L}_{iter}$ denotes
an LLM initialized with specific instructions to facilitate the optimization mapping. The function $\mathfrak{L}_{iter}$ is an instance of a general LLM $\mathfrak{L}$ and 
\begin{equation}
    \mathfrak{L}_{iter}(p_t) = \mathfrak{L}(p_t|m),
\end{equation}
where $m$ denotes the set of instructions, or a prompt, for optimizing $p_t$. In context of prompt optimization when we are talking about a prompt for optimizing another prompts, we call it a \textbf{meta-prompt}.
In this case, we can imagine $m$ as 
\begin{equation*}
    m \approx \text{Improve the following prompt \{insert\_prompt\}}.
\end{equation*}
By changing the metaprompt, one can easily change the nature of the optimization operator. In the following chapters, we will examine options for designing such operators and 
compare their performance and effectivity on industry-standard datasets, as well as on a brand new custom dataset.