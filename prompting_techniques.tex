\section{Prompting techniques}
\subsection{Prompt engineering}
Motivation for prompt engineering is to improve the model's capabilities not by changing the underlying weights with training on data but by crafting an optimal instruction string, or a prompt.
This can be done by providing examples of the task as a part of the prompt or by instructing the model how to solve the task.
In its essence, the model is a left-to-right text completion engine. We can make the analogy with human thinking modes, where it is said that humans
have a fast automatic "System 1" mode and a slow and deliberate "System 2" mode \cite{yao2023treethoughtsdeliberateproblem}. 
With a good prompt we can shift the model from "System 1" to "System 2".
\subsubsection{In-context learning}
Prompts are distinguished based on the number of included examples.
\begin{table}[h!]
    \centering
    \begin{tabular}{|c|p{12cm}|}
    \hline
    \textbf{Prompting Type} & \textbf{Description} \\
    \hline
    Zero-shot Prompting & Prompt has no examples, model relies on its pre-trained knowledge. \\
    \hline
    One-shot Prompting & Prompt has one example to guide the model. \\
    \hline
    Few-shot Prompting & Prompt includes a few examples (typically 2 to 5). \\
    \hline
    \end{tabular}
    \caption{Comparison of Zero-shot, One-shot, and Few-shot Prompting}
\end{table}        
Research\cite{brown2020languagemodelsfewshotlearners} has shown that with growing model size the knowledge-generalizing ability of the model increases. Instead of expensive fine-tuning
models can reuse knowledge from pre-training and solve many tasks when provided just by a few examples.
\subsection{Prompting techniques}
\subsubsection{Chain-of-thought (CoT)}
Instructing the model to "think step-by-step" results significant improvements on many benchmarks. Some research has shown that this approach hurts performance on some tasks where humans perform better without thinking.
Apart from more compute time being allocated at inference, the model also benefits from having its whole reasoning chain as a part of the context when writing the final answer to the task.
Several variants exist, such as CoT with self-consistency, where the final answer is acquired by majority-voting from several thinking chains.
\subsubsection{Reflexion}
Model reflects on its response and improves it.
\subsubsection{ReAct}
Multi-turn prompting technique that forms the basis of agentic LLMs. The model is given a set of tools, such as a Wikipedia search function or a math expression evaluator.
The model can go through several steps of using the tools, which generates "observation". The model uses these observations to generate a final answer and leaves the ReAct chain when ready using a "finish" function.
\subsubsection{Tree-of-thought}
Similar to CoT with self-consistency but the reasoning chain is split up into steps creating a tree. This reasoning tree can then be explored using a graph search algorithm such as DFS.
