\section{Prompt engineering}\label{sec:preng}
Maintaining the notation outlined in \ref{sec:notation}, prompt $p = \mathbf{i}(q)$ is a combination 
of a set of instruction $\mathbf{i}$ and a query $q$. 

By prompt engineering we mean crafting a instruction set which 
transforms the query into a result according to our task requirements.
Our task requirements can for example be
\begin{itemize}
    \item obtaining the correct answer for a mathematical problem
    \item fixing a bug in a code base
    \item explaining the contents of an image.
\end{itemize}
Each of these tasks needs a separate instruction set $\mathbf{i}$ which can then be used with multiple queries,
representing specific task instances. This signifies a shift from the training and fine-tuning paradigm, where 
a base model is first trained on a large corpus of data and then adapted for a specific task with supervised fine-tuning.
This process requires a substantial amount of training data and computation power, making specialized LLMs unsuitable
for many users and for applications, where extensive data collection is infeasible.


Since the inception of modern LLMs, prompt engineering has evolved into a field of its own. Current LLM systems, often containing
multiple chained and interlinked models, require robust and well thought-out prompts at each step. 
Indeed, in many modern LLM applications, prompts have become programs themselves\cite{schnabel2024symbolicpromptprogramsearch}, 
marking a huge leap from the basic text messages of the early LLM days.

In this section, we will briefly cover the most notable prompt engineering techniques, which we will then
be able to utilize in our study of automatic prompt optimization.
\newpage
\subsection{Components of a prompt}
We can dissect a prompt into several components\cite{schulhoff2024promptreportsystematicsurvey}.
\begin{itemize}
    \item \textbf{Directive:} The main task of the prompt, e.g., \textit{"Write an email to a coworker."}
    \item \textbf{Context:} Everything necessary or beneficial to completing the directive, e.g., \textit{"I was supposed to send a report to my boss, but I forgot."}
    \item \textbf{Examples:} How you would have solved a similar task, e.g. a past email on a similar topic.
    \item \textbf{Output specifications:} Style and format instructions, e.g., \textit{"Respond with three paragraphs in formal style with tasteful emojis."}
\end{itemize}

Although flexible and sometimes blended together, high-performing prompts \\often follow this structure and order. 
In more technical applications, it is beneficial to use tags or delimiters to explicitly separate components.
Models often reward prompts with a more code-like structure\cite{10.1145/3544548.3581388}. 

We now discuss each component in detail.

\subsubsection{Directive}
The directive should be a clear and objective description of the task, assuming that the model
already knows how to solve it\cite{reynolds2021promptprogramminglargelanguage}. 
Specific requirements that narrow the scope should be avoided and left for other components.

In cases where the prompt serves as a prompt template, meaning it can be reused with various data points,
the directive can include a placeholder. For example consider this directive
\begin{promptbox}{Directive with a placeholder}
    Write a limerick about \texttt{$\{$topic$\}$}.
\end{promptbox}
This directive, and the prompt to which it belongs, could be reused for multiple values of \texttt{topic}.

\subsubsection{Context}
Context should provide all the background information relevant to the task at hand.
The user can include more information about why they are using the model for the task,
define the target audience or attach relevant documents. For example, a prompt that asks the LLM to summarize an article:

\begin{promptbox}[label={box:contextprompt}]{Using context to personalize output}
    \textbf{Directive:}
    
    \hlbox{ctublue}{
    Summarize this article on climate change for a high-school debate. \\
    \texttt{$\{$article$\}$}
    }
    
    \textbf{Context:}
    
    \hlbox{ctublue}{
    I already understand the basic causes of climate change, but I struggle with the economic side. Focus on how it affects economies and give arguments I could use in a policy debate.
    }
\end{promptbox}

In this example, the initial instruction serves as the directive, while the second component provides contextual information about the user's prior knowledge and objectives.
Without this context, the model may produce a generic summary, overlooking the user's interest in economic impacts and failing to surface arguments relevant for a policy-oriented debate.

The context can become the endpoint for retrieval pipelines, which search a data source for 
relevant documents, or memory mechanisms, which gather personal information about the user from other conversations. 

Another possible feature of the context component is the use of a memetic proxy\cite{reynolds2021promptprogramminglargelanguage},
like role-assignment. Instead of writing a long instruction covering all the requirements and assumptions 
behind someone being an "experienced business analyst", we can just say "You are an experienced business analyst".
In this way, we can instruct the model to adopt an identity or an expertise level.
This primes the model to use a more technical language in its response.

\subsubsection{Examples}\label{sec:icl}
By providing examples of solutions to similar tasks, we can condition the LLM to generate
further examples from that distribution\cite{meyerson2024languagemodelcrossovervariation}, increasing the 
chance of a suitable completion. Furthermore, examples make the decoding more robust and decrease prompt sensitivity\cite{zhuo2024prosaassessingunderstandingprompt}.
\begin{table}[ht!]
    \centering
    
    \begin{tabular}{ c p{8cm} }
        \hline
        \textbf{Prompting Type} & \textbf{Description} \\
        \hline
        Zero-shot Prompting & Prompt has no examples. Model relies on instructions and pre-trained knowledge. \\
        
        One-shot Prompting & Prompt has one example to guide the model. \\
        
        Few-shot Prompting & Prompt includes a few examples. \\
        \hline
    \end{tabular}
    \caption{Comparison of Zero-shot, One-shot, and Few-shot Prompting}\label{tab:nshot}
\end{table}        
Table \ref{tab:nshot} shows the agreed-upon terminology for prompts with examples. 
The concept of adding examples to the prompt is also called In-Context Learning (ICL).
In some cases, Few-shot prompting can be effective even without the use of other instructions\cite{brown2020languagemodelsfewshotlearners}.
When adding examples to a prompt, we have to pay attention to several aspects\cite{schulhoff2024promptreportsystematicsurvey}.
\begin{itemize}
    \item \textbf{Exemplar quantity}: More is better with diminishing returns.
    \item \textbf{Exemplar ordering}: Models tend to pay more attention to the last examples.
    \item \textbf{Exemplar label distribution}: Unbalanced labels in examples skew model generation.
    \item \textbf{Exemplar label quality}: It is unclear whether incorrect examples hurt performance.
    \item \textbf{Exemplar format}: Optimal format may vary across tasks.
    \item \textbf{Exemplar similarity}: Effect of exemplar similarity depends on the situation.
\end{itemize}
\newpage
Contrary to Brown et al.\cite{brown2020languagemodelsfewshotlearners} who interpret the effectiveness of
few-shot prompting as the model learning the task by observing examples, later research\cite{reynolds2021promptprogramminglargelanguage}
suggestes that examples merely allow the LLM to locate the task more precisely in its learned task space.
Still, the authors argued for the use of examples for redundancy enforcing the desired behavior\cite{reynolds2021promptprogramminglargelanguage}.

\subsubsection{Output specifications}
With this component, we guide the structure, tone, style and formatting of the LLM's answer. 
A common technique, discussed in \ref{sec:cot} is inducing reasoning
with a CoT prompt, like "Let's think step-by-step", or instructing the LLM to 
first plan the solution and then execute it. 

We can influence the length of the answer, ask the model to be formal or humorous,
request specific formatting, like the use of \LaTeX{} equations, or have it answer in a JSON format
for a machine-readable response. Users should test multiple configurations as changing the output format
can influence the final prediction\cite{salinas2024butterflyeffectalteringprompts}.

\subsection{Meta-prompting}
Before we proceed we first need to overview the terminology discrepencies in contemporary research. 
Meta-prompts (also meta prompts or metaprompts) were first coined by Reynolds and McDonell\cite{reynolds2021promptprogramminglargelanguage}. 
Since then, this term was used in multiple contexts.
\begin{enumerate}
    \item \textbf{Task-agnostic zero-shot prompt:} In contrast to task-sample specific few-shot prompting, 
    meta-prompts were used to mean a 'task-agnostic zero-shot prompt', or 'seeds encapsulating a more general intention that will unfold into
    a specific prompt when combined with the task question\cite{reynolds2021promptprogramminglargelanguage}. 
    \item \textbf{Natural language metaprocedure:} Extending 1, Zhang et al.\cite{zhang2025metapromptingaisystems} define meta-prompts as example-agnostic prompts
    designed to capture the reasoning structure of a specific category of tasks. They do this by employing a typed and structured prompt, 
    resembling control flow templated and often expressed in JSON-like structures.
    This is similar to how DSPy\cite{khattab2023dspycompilingdeclarativelanguage} implements LLM calls with function signatures.
    \item \textbf{Soft prompt optimization method:} In a parallel branch of research, MetaPrompting\cite{hou2023metapromptinglearninglearnbetter} 
    was used as a name of a soft token prompt optimization method.
    \item \textbf{Prompt generator:} In automatic prompt engineering and prompt optimization literature, meta-prompt is a prompt that generates prompts\cite{dewynter2024metaprompting}.
    This can be seen as application of 2 to the task of prompt generation\cite{zhang2025metapromptingaisystems}.
\end{enumerate}

While both 1 and 2 are of interest to us, in this thesis, we will treat meta-prompts as prompt generators. 

\subsubsection{Meta-prompt as a prompt generator}
Prompt optimization literature\cite{ramnath2025systematicsurveyautomaticprompt} treats meta-prompts just a prompt which generates prompts.
As prompt generation is a complex reasoning-intensive language generation task\cite{ye2024promptengineeringpromptengineer}, 
all the principles regarding prompt design for other hard tasks apply to meta-prompts as well.

Meta-prompts are usually templates for other data, such as examples of the task for Instruction Induction\cite{honovich2022instructioninductionexamplesnatural},
past prompt generations along with their scores\cite{yang2024largelanguagemodelsoptimizers} and critiques of prompt outputs\cite{he2024crispomultiaspectcritiquesuggestionguidedautomatic}
or text description of the task\cite{ye2024promptengineeringpromptengineer}. Some reseach also utilizes professional task advice\cite{ramnath2025systematicsurveyautomaticprompt}
or a prompt engineering tutorial\cite{ye2024promptengineeringpromptengineer}. Other methods used seed phrases, like a thinking 
style\cite{fernando2023promptbreederselfreferentialselfimprovementprompt}, to steer the generation.

In one of the few theoretically rigorous works on the subject, de Wynter et al.\cite{dewynter2024metaprompting} formalize meta-prompts
using category theory. Showing the possibility of creating a general purpose meta-prompt as well as suggesting that such meta-prompt
will perform better than task-specific prompts in a wide range of applications.