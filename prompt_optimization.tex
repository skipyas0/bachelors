\section{Prompt optimization}
In a previous section on inference-time algorithms, we outlined several methods for improving the performance of LLM generation
and contrasted it to the traditional training-time scaling paradigm. In this section, we will discuss prompt optimization. 
This technique fits in neither training- or inference-time scaling techniques, as the computation allocated on
optimizing our prompts can then be amortized over multiple uses of the prompt. 
We will refer to this as compile-time\cite{schnabel2024symbolicpromptprogramsearch} scaling.

We will first discuss the motivation for automatizing the prompt engineering process. 
Prompt engineering is a largely empirical field without rigorous foundations.
Although research agrees on some best practices, creating effective prompts 
often requires substantial trial-and-error experimentation and deep task-specific 
knowledge\cite{xiang2025selfsupervisedpromptoptimization}.

As LLM use permeates into the general public, research\cite{10.1145/3544548.3581388} notes difficulties 
of the average user with prompt design and evaluation as users bring expectations from human-to-human 
interactions to prompt design. These include the expectation that semantically equivalent instructions 
should produce equivalent results. This however does not hold and even minute
alterations to the prompt, like adding a single space to the end, can dramatically influence the answer\cite{zhuo2024prosaassessingunderstandingprompt}\cite{salinas2024butterflyeffectalteringprompts}.
As another example in \ref{tab:cotprefixperf}, zero-shot CoT\cite{NEURIPS2022_8bb0d291} prefixes have vastly different performance on mathematical tasks.
\begin{table}[htbp]
    \centering
    \caption{Vastly different performance of CoT prefixes on GSM8K as per Yang et al.\cite{yang2024largelanguagemodelsoptimizers}}
    \label{tab:cotprefixperf}
    \begin{tabularx}{\linewidth}{Xc}
    \toprule
    \textbf{Prompt} & \textbf{GSM8K accuracy (\%)} \\
    \midrule
    ``Let's think step by step.'' & 71.8 \\
    ``Let's solve the problem together.'' & 60.5 \\
    ``Let's work together to solve this problem step by step.'' & 49.4 \\
    \bottomrule
    \end{tabularx}
\end{table}

The average user is not used to the meticulous process of systematic program crafting and debugging. It seems that
some coding proficiency is, in a way, a prerequisite for prompt engineering. With these limitations barring the general public from utilizing LLMs to their full potential,
automatic prompt engineering and prompt optimization (PO) have become an exceptionally active research field in the recent years.
\newpage
We can divide the PO research into two distinct branches depending on whether 
they treat prompts as sequences of discrete tokens or as soft embedding vectors. 
Both approaches have their pros and cons, summarized in table \ref{tab:disxcont}.
\begin{table}[htbp]
    \centering
    \caption{Comparison of Soft vs. Discrete Prompt Optimization}\label{tab:disxcont}
    \begin{tabular}{lcc}
    \toprule
    \textbf{Feature} & \textbf{Soft Optimization} & \textbf{Discrete Optimization} \\
    \midrule
    Gradient use      & $\checkmark$   & $\times$ \\
    Interpretability  & $\downarrow$ & $\uparrow$ \\
    Transferability  & $\downarrow$ & $\uparrow$ \\
    Cost  & $\downarrow$ & $\uparrow$ \\
    Usable with APIs  & $\times$   & $\checkmark$ \\
    \bottomrule
    \end{tabular}
    \end{table}

Although soft PO is more effective as it allows for the use of 
continuous optimization methods such as gradient descent, with the increasing size of models
it gets more expensive akin to fine-tuning. Furthermore, many of the most capable models are 
only accessible through proprietary APIs, which rarely allow access to the inner states of the LLM,
rendering soft PO unusable.
Optimizing prompts for an LLM hidden behind an API is inherently a black-box problem. 
In this thesis, we will study methods of discrete PO.

Table \ref{tab:optsurvey} offers an overview of discrete PO literature. Note that only only selected articles are included based on
\begin{enumerate}
    \item being somewhat comparable,
    \item being relevant to the implementation part of this thesis.
\end{enumerate}
Also note that, due to space-constraints, only the most notable aspects of the methods are included. 
For a more comprehensive overview of PO methods, we refer the reader to a survey\cite{ramnath2025systematicsurveyautomaticprompt} by Ramnath et al.

\newcolumntype{Y}{>{\raggedright\arraybackslash}p{4.25cm}}     
\begin{landscape}
\rowcolors{2}{ctubluebg}{white}
\begin{table}[htbp]
\centering
\caption{Survey of Discrete PO Methods.}\label{tab:optsurvey}
\begin{tabularx}{\linewidth}{@{}l Y Y Y Y@{}}
\toprule
\textbf{Method} & \textbf{Initialization} & \textbf{Selection} & \textbf{Expansion} & \textbf{Notes} \\
\midrule

APE\cite{zhou2023largelanguagemodelshumanlevel} & instruction induction & filter top-k prompts & paraphrasing & iterating does not help \\
ProTeGi\cite{pryzant2023automaticpromptoptimizationgradient} & manual initial prompt & UCB Bandits, Succesive Rejects & critique of a prompt as a "gradient" & state-of-the-art according to\cite{wan2024teachbettersmarterinstructions} \\
OPRO\cite{yang2024largelanguagemodelsoptimizers} & baseline prompt, instruction induction & filter top-k prompts & meta-prompt with scored prompts and exemplars & high sampling temperature for diversity \\
Promptbreeder\cite{fernando2023promptbreederselfreferentialselfimprovementprompt} & seeded mutation of problem description & binary tournament & random operator out of 9 total & mutates meta-prompt, ICL examples \\
Evoprompt\cite{guo2024connectinglargelanguagemodels} & manual prompts and instruction induction & roulette and filter based on score & crossover and mutation & two operator variants \\
PE2\cite{ye2024promptengineeringpromptengineer} & manual or instruction induction & filter top-k prompts & meta-prompt with step-by-step instructions & optional task examples in meta-prompt \\
PhaseEvo\cite{cui2024phaseevounifiedincontextprompt} & manual or instruction induction & no filtering, success vector Hamming distance & instruction induction, EDA, crossover, feedback, paraphrase & alternates local, global search \\
PromptWizard\cite{agarwal2024promptwizardtaskawarepromptoptimization} & variations from problem description & selects best prompt & critique + synthesis & improves examples after instructions \\
CriSPO\cite{he2024crispomultiaspectcritiquesuggestionguidedautomatic} & manual initial prompt & filter top-k prompts & meta-prompt with critique history & self-generates critique aspects \\
SPRIG\cite{zhang2024sprigimprovinglargelanguage} & blank prompt & filters tens of thousands of candidates with UCB & edit-based enumeration of sentence-level operations & focus is on task-agnostic system prompts \\
SPO\cite{xiang2025selfsupervisedpromptoptimization} & basic prompt template (e.g. CoT) & winner of pairwise evaluation & metaprompt utilizing feedback from evaluation & focus is on reference-free optimization\\
\bottomrule
\end{tabularx}
\end{table}
\end{landscape}

We can further divide the discrete PO research into two branches, depending on whether 
the method optimizes instructions (IO) or examples (EO). While some methods jointly optimize both, most
research focuses on optimizing either one or optimizes them independently. 

\subsection{Exemplar optimization}

Exemplar optimization (EO) focuses on selecting the most effective demonstrations for ICL, a powerful technique we discussed in \ref{sec:icl},
where the LLM learns the task implicitly from labeled input-output pairs. Although underrepresented in literature\cite{wan2024teachbettersmarterinstructions},
recent work\cite{opsahlong2024optimizinginstructionsdemonstrationsmultistage}\cite{wan2024teachbettersmarterinstructions} shows that intelligently selected
exemplars often outperform optimized instructions alone and even simple optimization methods like random search can lead to significant gains across diverse tasks.
This effect is further amplified when EO is used together with IO, suggesting that the two should be co-optimized rather than treated separately\cite{wan2024teachbettersmarterinstructions}

EO is useful even for tasks without available examples with techniques like Bootstrapping\cite{khattab2023dspycompilingdeclarativelanguage},
which utilizes inputs solved during optimization. By indentifying useful and informative solutions, it allows for EO without hand-crafted input-output pairs.


\subsection{Instruction optimization}
The problem of finding the optimal instruction set can be formulated as a natural language program synthesis problem\cite{zhou2023largelanguagemodelshumanlevel}.
Although natural language program formulation is favorable as it represents a 
natural interface for humans to communicate with machines, it brings its own set of problems.
Compared to automatic program synthesis methods, the search space of natural language is even larger. 
This makes finding the right instructions extremely difficult. 

With continuous optimization, minor perturbations of e.g. network weights generates predictably small changes in functionality.
Discrete changes on the other hand often dramatically change functionality\cite{lehman2022evolutionlargemodels} 
and are not amenable to gradient-based optimization\cite{deng2022rlpromptoptimizingdiscretetext}. 


\subsubsection{Enumeration-based approaches}
First discrete PO research such as APE\cite{zhou2023largelanguagemodelshumanlevel} 
relied on Monte Carlo search, based on the idea of Instruction Induction\cite{honovich2022instructioninductionexamplesnatural}.
This concept works by reverse-engineering instructions from a few task samples with a meta-prompt template like in \ref{box:insinductexample}. 


\begin{promptbox}
    [label={box:insinductexample}]{Instruction Induction}
    \textbf{LLM Input:} \hlbox{ctublue}{Below are a few examples of a task. \newline Write a set of instructions that would help me solve other examples of the task. 
    \hlbox{ctulightblue}{\textbf{Q:} Jim earns 10 dollars per hour as a waiter. How much does he earn for 5 hours and 45 minutes of work? \\
    \textbf{A:} Jim earns $5*10=50$ dollars for the full 5 hours and $10*\frac{3}{4}=7.5$ dollars for the 45 minutes. That makes $50+7.5=57.5$ dollars in total. \#\#\#\# 57.5 \\
    \textbf{Q:} ... \textbf{A:} ... }}
    \textbf{LLM Output:}
    \hlbox{ctublue}{Use step-by-step logical thinking to solve this mathematical word problem. Show your work and write your numerical answer separated by '\#\#\#\#'.}
    
\end{promptbox}

By repeating this process with a non-zero sampling temperature while varying the example set,
the LLM produces a diverse set of prompts. APE then scores them on a validation set and selects the instructions with the best performance.
Extending this method, iterative APE explores local space around promising prompts by paraphrasing them.
This however brought marginal performance gains in comparison to the basic Instruction Induction sampling\cite{zhou2023largelanguagemodelshumanlevel}.
Deng et al.\cite{deng2022rlpromptoptimizingdiscretetext} argue that this is because "paraphrasing-then-selection" methods do not explore the prompt space systematically.

Although the research has moved onto more advanced iterative optimization methods, Instruction Induction remains one of the 
most common ways of initialization in literature. The alternatives are manual initialization, which 
either consists of a basic prompt, or expertly prompt-engineering prompts. 
The former poses a problem, as starting the search with high-quality instructions 
is essential due to the intractably large search space\cite{opsahlong2024optimizinginstructionsdemonstrationsmultistage}. 
The latter is beneficial as it allows the optimization to leverage human knowledge\cite{guo2024connectinglargelanguagemodels}, but
to an extent defeats the purpose of automatic PO. Also, it is impossible to have pre-defined expert prompts for tasks that are not known yet.

\subsubsection{"Gradient"-based approaches}
Although gradient-based optimization is unavailable in the discrete text space, many prompt optimization methods try to approximate with,
as Tang et al.\cite{tang2024unleashingpotentiallargelanguage} put it, analogical gradient forms. Gradient forms provide a clear
optimization direction in a hill-climber setting and allow the use gradient descent strategies such momentum, step size, warm-up and decay.

The simplest gradient form is a series of prompts and their scores in a meta-prompt instructing the optimizer LLM
to create a new prompt in the sequence. The hope is for the model to extrapolate beyond the sequence of prompts
and apply the observed pattern to acquire a better performing prompt. 

Ablations in OPRO\cite{yang2024largelanguagemodelsoptimizers} show that both the ascending order of the prompts
as well showing scores in beneficial to the optimization process. Furthermore, another crucial part
of the meta-prompt according to \cite{yang2024largelanguagemodelsoptimizers} and \cite{tang2024unleashingpotentiallargelanguage} are examples of the task similar to the
Instruction Induction meta-prompt. This helps the LLM better understand the task at hand. An alternative would be to include a description of the task\cite{he2024crispomultiaspectcritiquesuggestionguidedautomatic}.

\begin{promptbox}
    [label={box:oprometa}]{Prompt+Score Meta-prompt}
    \textbf{LLM Input:} \hlbox{ctublue}{Your task is to create a new prompt for a language model. \\
    Below is a sequence of past prompts and their scores.\\
    Design a new prompt in the sequence so that it achieves a better score. 
    \hlbox{ctulightblue}{\textbf{Prompt $1$:} ''Do the math.'' \hfill \textbf{Score}: 34.5\;\\
     ...\\
     \textbf{Prompt $n$:} ''Let's think step-by-step.'' \hfill \textbf{Score}: 70.3\;
    }
    \hlbox{ctuorange}{\textbf{Optional:} Examples of the task like in \ref{box:insinductexample} }
    }
    \textbf{LLM Output:}
    \hlbox{ctublue}{Plan and solve the challenge while showing your work.}
\end{promptbox}


Another branch of research, e.g. ProTeGi\cite{pryzant2023automaticpromptoptimizationgradient} and CriSPO\cite{he2024crispomultiaspectcritiquesuggestionguidedautomatic}
focus on using model feedback as a part of the optimization signal. Outputs of LLMs contain rich quality 
information that directly reflects prompt effectiveness\cite{xiang2025selfsupervisedpromptoptimization}. 
Utilizing this information makes for a stronger optimization signal than just a numerical score.
This is pronounced particularly for text-generation tasks, where applying PO methods based on prompt+score pairs 
is challenging due to the lack of effective optimization signals\cite{he2024crispomultiaspectcritiquesuggestionguidedautomatic}.
A single number does not capture the nuances of text and opportunities for improvement. 

Pryzant et al.\cite{pryzant2023automaticpromptoptimizationgradient}
compare the prompt critique to a gradient in the text space. The LLM can in principle, thanks to its human-like task comprehension\cite{xiang2025selfsupervisedpromptoptimization},
perform a sort of gradient descent by fixing the issues found by the critique. The LLM outputs on a task can be viewed as an environment observation - an extrinsic information source.
This is, as we discussed in \ref{sec:inference}, crucial for the LLM's ability to self-refine. 
Feedback-enriched meta-prompts, such as \ref{box:crispometa}, may include prompt scores as well.

\begin{promptbox}
    [label={box:crispometa}]{Prompt+Feedback+Score Meta-prompt}
    \textbf{LLM Input:} \hlbox{ctublue}{Your task is to create a new prompt for a language model. \\
    Below is a sequence of past prompts and their critiques and scores.\\
    Design a new prompt in the sequence so that it achieves a better score. 
    \hlbox{ctulightblue}{\textbf{Prompt $1$:} ''Do the math.'' \hfill \textbf{Score}: 34.5\;\\
     \textbf{Critique:} Does not encourage step-by-step reasoning.\\
     ...\\
     \textbf{Prompt $n$:} ''Let's think step-by-step.'' \hfill \textbf{Score}: 70.3\;
     \textbf{Critique:} Too general, could be more enthusiastic.
    }
    }
    \textbf{LLM Output:} 
    \hlbox{ctublue}{Yay! Let's solve this math problem with logical thinking! }
\end{promptbox}

The instructions in \ref{box:oprometa} and \ref{box:crispometa} are rather basic and can be endlessly
improved via prompt engineering. For example Ye et al.\cite{ye2024promptengineeringpromptengineer} attempt to 
improve on APE and ProTeGi by designing a more complete meta-prompt. 

By extending the gradient analogy, Tang et al.\cite{tang2024unleashingpotentiallargelanguage} introduce other concepts
known from traditional machine learning. First, the learning rate can be implemented by limiting the number of token, word or sentence edits the model
can make. Next, we can encorporate variable learning rate with strategies such as warm-up (learning rate grows in the beggining) or decay (learning rate
gets smaller towards the end).

Another way of balancing exploration and exploitation, besides edit limit-based learning rate analogies, is tuning the LLM sampling temperature . 
Lower temperature encourages exploitation in the local solution space and higher temperature allows 
more aggressive exploration of different solutions\cite{yang2024largelanguagemodelsoptimizers}.

By encorporate a history of past prompts in the metaprompt, we implement an analogy of momentum, a concept 
from machine learning which utilizes past gradients.
Including the optimization trajectory is useful but poses the problem of inflating the meta-prompt. 
This means higher costs per prompt generation, but also risks of surpassing the LLM's context length limit.
To fit into the context limit, trajectory can be summarized or retrieved based on recency, relevance or importance\cite{tang2024unleashingpotentiallargelanguage}.


\subsubsection{Evolution-based approaches}
Evolutionary Algorithms (EAs) are a time-proven and versatile optimization method. 
They have been shown to be effective in search spaces with millions and billions of variables\cite{meyerson2024languagemodelcrossovervariation}
by emulating the processes observed in natural evolution using crossover (sexual reproduction) and mutation (asexual reproduction) operators.
Although widely successful, EAs have been limited by the challenging nature of operator design.
Developing them requires extensive manual crafting with domain knowledge\cite{liu2024largelanguagemodelsevolutionary}.

With the advent on LLMs and few-shot prompting, their pattern-completion ability can be leveraged \\to create a form of intelligent 
evolutionary crossover\cite{meyerson2024languagemodelcrossovervariation}, which can be in theory truly general.
In principle, LLMs can mutate and combine any text representation that has moderate support in its training dataset
and perform any genetic operator through fine-tuning or prompt engineering.
The intersection between LLMs and EAs, among other traditional algorithms, emerges as an exciting branch of research.

It lends itself to leverage LLM-powered EAs to improve prompts for the LLM, extending previously
discussed PO methods. The seminal work in this space was EvoPrompt\cite{guo2024connectinglargelanguagemodels},
which repurposed two EAs: Genetic Algorithm and Differential Evolution for PO. 
Treating the text sequences in prompts as gene sequences, EvoPrompt performes crossover and mutation
on text prompts. 

EA-based PO seems to demostrate lower sensitivity to initial prompts. Although EvoPrompt utilizes some manual prompts,
it achieves similar results with randomly sampled initial population as when using the best prompts\cite{guo2024connectinglargelanguagemodels}.
On the other hand, they suffer from extremely high computational cost and slow convergence speed\cite{cui2024phaseevounifiedincontextprompt}.

Methods like PhaseEvo\cite{cui2024phaseevounifiedincontextprompt} and Promptbreeder\cite{fernando2023promptbreederselfreferentialselfimprovementprompt} strive to
improve the efficiency and convergence of EA-based PO methods by supercharging them with more operators to provide balance between exploration and exploitation.
Cui et al.\cite{cui2024phaseevounifiedincontextprompt} argue that the standard EA operators prioritize exploration and categorize them as global operators.
To supplement them, they introduce local operators based on feedback, optimization trajectory and paraphrasing. By alternating between global and local 
search, PhaseEvo demostrates better cost-efficiency compared to other EA-based PO methods\cite{cui2024phaseevounifiedincontextprompt}. 

In Promptbreeder\cite{fernando2023promptbreederselfreferentialselfimprovementprompt}, authors also include multiple different operators.
Besides variations of previously discussed operators, they also include shuffling ICL examples and, most notably, a "hyper-prompt" which mutates 
the optimizer meta-prompt in hope to make the method self-referential. A possible flaw in this method is the fact that it
selects the operators randomly at each step, leading to diminished efficiency compared to a more organized approach\cite{cui2024phaseevounifiedincontextprompt}
and hundreds of evaluations necessary before convergence\cite{wan2024teachbettersmarterinstructions}.

All three discussed EA-based methods utilize Instruction Induction\cite{honovich2022instructioninductionexamplesnatural} to some extent, usually
using calling it "Lamarckian mutation". Besides initialization, Lamarckian mutation can be useful for adding task-relevant prompts
to the population in case the optimization diverges\cite{fernando2023promptbreederselfreferentialselfimprovementprompt}. 

\begin{promptbox}
    [label={box:mutate}]{Mutation Meta-prompt}
    \textbf{LLM Input:} \hlbox{ctublue}{Rewrite the prompt below in a semantically equivalent but novel way.
    \hlbox{ctulightblue}{
     \textbf{Prompt:} ''Let's think step-by-step.'' 
    }
    }
    \textbf{LLM Output:} 
    \hlbox{ctublue}{We will solve this in logical increments.}
\end{promptbox}

\begin{promptbox}
    [label={box:crossover}]{Crossover/EDA Meta-prompt}
    \textbf{LLM Input:} \hlbox{ctublue}{Combine the following prompts into a novel prompt. 
    \hlbox{ctulightblue}{\textbf{Prompt $1$:} ''Do the math.''\\
     ...\\
     \textbf{Prompt $n$:} ''Let's think step-by-step.''
    }
    }
    \textbf{LLM Output:}
    \hlbox{ctublue}{Do the step-by-step thinking and solve the math problem.}
\end{promptbox}

In \ref{box:mutate} and \ref{box:crossover} the reader can find possible meta-prompts for a mutation and a crossover operater, respectively.
By changing the meta-instructions in \ref{box:crossover} and increasing $n$, we can shift the crossover operator into a Estimation-of-distribution (EDA)
operator. Whereas crossover aims to combine 2-3 prompts into 1, EDA infers the distribution of a larger number of prompts and tries to 
create a new prompt from that distribution. 

An important factor in the implementation of the crossover and EDA operators is the way of selecting prompt specimens.
In EvoPrompt\cite{guo2024connectinglargelanguagemodels}, a roulette selection method is employed. Other methods utilize more advanced
selection methods to encourage diversity. Promptbreeder\cite{fernando2023promptbreederselfreferentialselfimprovementprompt} filters inputs
for its EDA based on their BERT embedding and PhaseEvo\cite{cui2024phaseevounifiedincontextprompt} selects parent prompts
based on the Hamming distance of their "performance vectors", which hold the prompts' performance on task samples.  
This way ensures that the prompts that get paired with each other do not make the same mistakes.

Another important factor in the design of the operator is the ordering. Both Promptbreeder and PhaseEvo sort 
the prompts in ascending order of fitness. To prevent the model from relying too much on the last prompts, the authors
lie to the model by telling it the prompt are in descending order of fitness. 
This somewhat curbs the bias toward the later examples\cite{cui2024phaseevounifiedincontextprompt}.

EAs are but one of many metaheuristics historically used for optimization and the 
interactions of LLMs and metaheuristics is poised to be a fruitful research area in the near future.
In their research Pan et al.\cite{pan2024plumpromptlearningusing} used and compared several metaheuristics
including Hill-Climbing, Simulated Annealing, Genetic Algorithm, Tabu Search and Harmony Search for PO.


\subsubsection{Note on evaluation cost reduction} 
In many PO methods, prompt candidate evalutation is the most computation-intensive part of the process.
Especially with score-based evaluation, it has to be performed multi times to ensure scoring stability\cite{xiang2025selfsupervisedpromptoptimization}.
To lower the evaluation costs, research adopts two approaches. 

First branch, represented by ProTeGI\cite{pryzant2023automaticpromptoptimizationgradient} and SPRIG\cite{zhang2024sprigimprovinglargelanguage} use strategies such as
UCB and Succesive Rejects to allocate evaluations only to promising candidates. This way, the total compute budget gets reduced.
The other branch relies on ditching the numerical scoring and comparing outputs directly in a pairwise manner with a LLM judge\cite{xiang2025selfsupervisedpromptoptimization}.

\subsection{Multi-stage and reference-free methods}

Common critique of the methods is that they are unpractical and not applicable to real-world LLM use cases.
Most methods depend heavily on external references for evaluation which are often unavailable or 
unpractical to define especially for open-ended tasks \cite{xiang2025selfsupervisedpromptoptimization}.
Xiang et al.\cite{xiang2025selfsupervisedpromptoptimization} tackle this problem by using pairwise LLM-based evaluation
and Zhang et al.\cite{zhang2024glapegoldlabelagnosticprompt} develop a gold label-free method method on evaluation based
on self-consistencies of different answers.

Also, as the complexity of prompt structure increases, many prompt optimization techniques are no longer applicable\cite{schnabel2024symbolicpromptprogramsearch}.
Schnabel et al.\cite{schnabel2024symbolicpromptprogramsearch} define Symbolic Prompt Programs, representable
as directed acyclic graphs. In these graphs, nodes are functions, such as \texttt{RenderText}, \texttt{GenerateResponse} or \texttt{ParseOutput} 
and edged are dependencies between these nodes. This representation allows for effective search with node mutators
and a multitude of search algorithms.

Furthermore, modern LLM workflows interconnect various LLM and prompt instances into complex prompt programs.
Most prompt optimizer approaches do not apply to these multi-stage LLM programs\cite{opsahlong2024optimizinginstructionsdemonstrationsmultistage}
as a whole. Optimizing each component separately is possible but this approach ignores their mutual influence.

DSPy\cite{khattab2023dspycompilingdeclarativelanguage} is a Python library aiming to simplify LLM program composition with 
an intuitive interface. Most importantly, it allows for optimizing the whole pipeline, including EO, IO and weight fine-tuning
with the promise of declarative LLM program design without extensive prompt engineering. DSPy offers several pipeline optimization
methods, like MiPROv2\cite{opsahlong2024optimizinginstructionsdemonstrationsmultistage} and BetterTogether\cite{soylu2024finetuningpromptoptimizationgreat}.
MiPROv2 generalizes OPRO\cite{yang2024largelanguagemodelsoptimizers} to multi-stage joint example and instruction optimization
utilizing a surrogate Bayesian model to find the optimal configuration. With BetterTogether, Soylu et al. show that PO and fine-tuning
complement each other, achieving superior performance.

