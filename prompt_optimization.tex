\section{Prompt optimization}
\subsection{Soft prompt tuning}
Prompts for models which allow access to gradients, which is not the case for proprietary models accessed via APIs, can be optimized in the high-dimensional embedding space.
This makes the optimization problem continous. Soft prompts however pose the problem of interpretability and are non-transferable across different LLMs \cite{deng2022rlpromptoptimizingdiscretetext}.
\subsection{Discrete prompt tuning}
The area of optimizing prompts discretely while utilizing language models as optimization operators has attracted significant research interest in recent years.
\subsubsection{Textual gradients}
Naturally there are no gradients in the text space but some researchers try to emulate them using reflection-based operators.
\subsubsection{Evolutionary optimization}
Building upon the inherent ability of LLMs to paraphrase (mutation) and combine (crossover) text, an interesting intersection of traditional evolutionary algorithms and modern LLMs has formed. 
\subsubsection{Metaprompting}
Metaprompting or "prompting to create prompts". Research shows that meta-prompting will always be superior to prompting through category theory\cite{dewynter2024metaprompting}.