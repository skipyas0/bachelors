\section{Prompt optimization}
In a previous section on inference-time algorithms, we outlined several methods for improving the performance of LLM generation
and contrasted it to the traditional training-time scaling paradigm. In this section, we will discuss prompt optimization. 
This technique fits in neither training- or inference-time scaling techniques, as the computation allocated on
optimizing our prompts can then be amortized over multiple uses of the prompt. 
We will refer to this as compile-time\cite{schnabel2024symbolicpromptprogramsearch} scaling.

We will first discuss the motivation for automatizing the prompt engineering process. 
Prompt engineering is a largely empirical field without rigorous foundations.
Although research agrees on some best practices, creating effective prompts 
often requires substantial trial-and-error experimentation and deep task-specific 
knowledge\cite{xiang2025selfsupervisedpromptoptimization}.

As LLM use permeates into the general public, research\cite{10.1145/3544548.3581388} notes difficulties 
of the average user with prompt design and evaluation as users bring expectations from human-to-human 
interactions to prompt design. These include the expectation that semantically equivalent instructions 
should produce equivalent results. This however does not hold and even minute
alterations to the prompt, like adding a single space to the end, can dramatically influence the answer\cite{zhuo2024prosaassessingunderstandingprompt}\cite{salinas2024butterflyeffectalteringprompts}.
As another example in \ref{tab:cotprefixperf}, zero-shot CoT\cite{NEURIPS2022_8bb0d291} prefixes have vastly different performance on mathematical tasks.
\begin{table}[htbp]
    \centering
    \caption{Vastly different performance of CoT prefixes on GSM8K as per Yang et al.\cite{yang2024largelanguagemodelsoptimizers}}
    \label{tab:cotprefixperf}
    \begin{tabularx}{\linewidth}{Xc}
    \toprule
    \textbf{Prompt} & \textbf{GSM8K accuracy (\%)} \\
    \midrule
    ``Let's think step by step.'' & 71.8 \\
    ``Let's solve the problem together.'' & 60.5 \\
    ``Let's work together to solve this problem step by step.'' & 49.4 \\
    \bottomrule
    \end{tabularx}
\end{table}

The average user is not used to the meticulous process of systematic program crafting and debugging. It seems that
some coding proficiency is, in a way, a prerequisite for prompt engineering. With these limitations barring the general public from utilizing LLMs to their full potential,
automatic prompt engineering and prompt optimization (PO) have become an exceptionally active research field in the recent years.
\newpage
We can divide the PO research into two distinct branches depending on whether 
they treat prompts as sequences of discrete tokens or as soft embedding vectors. 
Both approaches have their pros and cons, summarized in table \ref{tab:disxcont}.
\begin{table}[htbp]
    \centering
    \caption{Comparison of Soft vs. Discrete Prompt Optimization}\label{tab:disxcont}
    \begin{tabular}{lcc}
    \toprule
    \textbf{Feature} & \textbf{Soft Optimization} & \textbf{Discrete Optimization} \\
    \midrule
    Gradient use      & $\checkmark$   & $\times$ \\
    Interpretability  & $\downarrow$ & $\uparrow$ \\
    Transferability  & $\downarrow$ & $\uparrow$ \\
    Cost  & $\downarrow$ & $\uparrow$ \\
    Usable with APIs  & $\times$   & $\checkmark$ \\
    \bottomrule
    \end{tabular}
    \end{table}

Although soft PO is more effective as it allows for the use of 
continuous optimization methods such as gradient descent, with the increasing size of models
it gets more expensive akin to fine-tuning. Furthermore, many of the most capable models are 
only accessible through proprietary APIs, which rarely allow access to the inner states of the LLM,
rendering soft PO unusable.
Optimizing prompts for an LLM hidden behind an API is inherently a black-box problem. 
In this thesis, we will study methods of discrete PO.

Table \ref{tab:optsurvey} offers an overview of discrete PO literature. Note that only only selected articles are included based on
\begin{enumerate}
    \item being somewhat comparable,
    \item being relevant to the implementation part of this thesis.
\end{enumerate}
Also note that, due to space-constraints, only the most notable aspects of the methods are included. 
For a more comprehensive overview of PO methods, we refer the reader to a survey\cite{ramnath2025systematicsurveyautomaticprompt} by Ramnath et al.

\newcolumntype{Y}{>{\raggedright\arraybackslash}p{4.25cm}}     
\begin{landscape}
\rowcolors{2}{ctubluebg}{white}
\begin{table}[htbp]
\centering
\caption{Survey of Discrete PO Methods.}\label{tab:optsurvey}
\begin{tabularx}{\linewidth}{@{}l Y Y Y Y@{}}
\toprule
\textbf{Method} & \textbf{Initialization} & \textbf{Selection} & \textbf{Expansion} & \textbf{Notes} \\
\midrule

APE\cite{zhou2023largelanguagemodelshumanlevel} & instruction induction & filter top-k prompts & paraphrasing & iterating does not help \\
ProTeGi\cite{pryzant2023automaticpromptoptimizationgradient} & manual initial prompt & UCB Bandits, Succesive Rejects & critique of a prompt as a "gradient" & state-of-the-art according to\cite{wan2024teachbettersmarterinstructions} \\
OPRO\cite{yang2024largelanguagemodelsoptimizers} & baseline prompt, instruction induction & filter top-k prompts & meta-prompt with scored prompts and exemplars & high sampling temperature for diversity \\
Promptbreeder\cite{fernando2023promptbreederselfreferentialselfimprovementprompt} & seeded mutation of problem description & binary tournament & random operator out of 9 total & mutates meta-prompt, ICL examples \\
Evoprompt\cite{guo2024connectinglargelanguagemodels} & manual prompts and instruction induction & roulette and filter based on score & crossover and mutation & two operator variants \\
PE2\cite{ye2024promptengineeringpromptengineer} & manual or instruction induction & filter top-k prompts & meta-prompt with step-by-step instructions & optional task examples in meta-prompt \\
PhaseEvo\cite{cui2024phaseevounifiedincontextprompt} & manual or instruction induction & no filtering, success vector Hamming distance & instruction induction, EDA, crossover, feedback, paraphrase & alternates local, global search \\
PromptWizard\cite{agarwal2024promptwizardtaskawarepromptoptimization} & variations from problem description & selects best prompt & critique + synthesis & improves examples after instructions \\
CriSPO\cite{he2024crispomultiaspectcritiquesuggestionguidedautomatic} & manual initial prompt & filter top-k prompts & meta-prompt with critique history & self-generates critique aspects \\
SPRIG\cite{zhang2024sprigimprovinglargelanguage} & blank prompt & filters tens of thousands of candidates with UCB & edit-based enumeration of sentence-level operations & focus is on task-agnostic system prompts \\
SPO\cite{xiang2025selfsupervisedpromptoptimization} & basic prompt template (e.g. CoT) & winner of pairwise evaluation & metaprompt utilizing feedback from evaluation & focus is on reference-free optimization\\
\bottomrule
\end{tabularx}
\end{table}
\end{landscape}

We can further divide the discrete PO research into two branches, depending on whether 
the method optimizes instructions (IO) or examples (EO). While some methods jointly optimize both, most
research focuses on optimizing either one or optimizes them independently. 

Although underrepresented in literature, EO provides superior performance than IO\cite{opsahlong2024optimizinginstructionsdemonstrationsmultistage}\cite{wan2024teachbettersmarterinstructions}.
The best performance is achieved when EO and IO are combined and should not be treated separately.

\subsection{Exemplar optimization}

As we discussed in \ref{sec:icl}, In-Context Learing (ICL) is a powerful way of conditioning the LLM to produce
task-appropriate outputs. Exemplar optimization aims to enrich the prompt by selecting a handful of optimal examples.



\subsection{Instruction optimization}
The problem of finding the optimal instruction set can be formulated as a natural language program synthesis problem\cite{zhou2023largelanguagemodelshumanlevel}.
Although natural language program formulation is favorable as it represents a 
natural interface for humans to communicate with machines, it brings its own set of problems.
Compared to automatic program synthesis methods, the search space of natural language is even larger. 
This makes finding the right instructions extremely difficult. 

With continuous optimization, minor perturbations of e.g. network weights generates predictably small changes in functionality.
Discrete changes on the other hand often dramatically change functionality\cite{lehman2022evolutionlargemodels} 
and are not amenable to gradient-based optimization\cite{deng2022rlpromptoptimizingdiscretetext}. 


\subsubsection{Enumeration-based approaches}
First discrete PO research such as APE\cite{zhou2023largelanguagemodelshumanlevel} 
relied on Monte Carlo search, based on the idea of Instruction Induction\cite{honovich2022instructioninductionexamplesnatural}.
This concept works by reverse-engineering instructions from a few task samples with a meta-prompt template like in \ref{box:insinductexample}. 


\begin{promptbox}
    [label={box:insinductexample}]{Instruction Induction}
    \textbf{LLM Input:} \hlbox{ctublue}{Below are a few examples of a task. \newline Write a set of instructions that would help me solve other examples of the task. 
    \hlbox{ctulightblue}{\textbf{Q:} Jim earns 10 dollars per hour as a waiter. How much does he earn for 5 hours and 45 minutes of work? \\
    \textbf{A:} Jim earns $5*10=50$ dollars for the full 5 hours and $10*\frac{3}{4}=7.5$ dollars for the 45 minutes. That makes $50+7.5=57.5$ dollars in total. \#\#\#\# 57.5 \\
    \textbf{Q:} ... \textbf{A:} ... }}
    \textbf{LLM Output:}
    \hlbox{ctublue}{Use step-by-step logical thinking to solve this mathematical word problem. Show your work and write your numerical answer separated by '\#\#\#\#'.}
    
\end{promptbox}

By repeating this process with a non-zero sampling temperature while varying the example set,
the LLM produces a diverse set of prompts. APE then scores them on a validation set and selects the instructions with the best performance.
Extending this method, iterative APE explores local space around promising prompts by paraphrasing them.
This however brought marginal performance gains in comparison to the basic Instruction Induction sampling\cite{zhou2023largelanguagemodelshumanlevel}.
Deng et al.\cite{deng2022rlpromptoptimizingdiscretetext} argue that this is because "paraphrasing-then-selection" methods do not explore the prompt space systematically.

Although the research has moved onto more advanced iterative optimization methods, Instruction Induction remains one of the 
most common ways of initialization in literature. The alternatives are manual initialization, which 
either consists of a basic prompt, or expertly prompt-engineering prompts. 
The former poses a problem, as starting the search with high-quality instructions 
is essential due to the intractably large search space\cite{opsahlong2024optimizinginstructionsdemonstrationsmultistage}. 
The latter is beneficial as it allows the optimization to leverage human knowledge\cite{guo2024connectinglargelanguagemodels}, but
to an extent defeats the purpose of automatic PO. Also, it is impossible to have pre-defined expert prompts for tasks that are not known yet.

\subsubsection{"Gradient"-based approaches}
Although gradient-based optimization is unavailable in the discrete text space, many prompt optimization methods try to approximate with,
as Tang et al.\cite{tang2024unleashingpotentiallargelanguage} put it, analogical gradient forms. Gradient forms provide a clear
optimization direction in a hill-climber setting and allow the use gradient descent strategies such momentum, step size, warm-up and decay.

The simplest gradient form is a series of prompts and their scores in a meta-prompt instructing the optimizer LLM
to create a new prompt in the sequence. The hope is for the model to extrapolate beyond the sequence of prompts
and apply the observed pattern to acquire a better performing prompt. 

Ablations in OPRO\cite{yang2024largelanguagemodelsoptimizers} show that both the ascending order of the prompts
as well showing scores in beneficial to the optimization process. Furthermore, another crucial part
of the meta-prompt according to\cite{yang2024largelanguagemodelsoptimizers} are examples of the task similar to the
Instruction Induction meta-prompt. This helps the LLM better understand the task at hand. 

\begin{promptbox}
    [label={box:oprometa}]{Prompt+Score Meta-prompt}
    \textbf{LLM Input:} \hlbox{ctublue}{Your task is to create a new prompt for a language model. \\
    Below is a sequence of past prompts and their scores.\\
    Design a new prompt in the sequence so that it achieves a better score. 
    \hlbox{ctulightblue}{\textbf{Prompt $1$:} ''Do the math.'' \hfill \textbf{Score}: 34.5\;\\
     ...\\
     \textbf{Prompt $n$:} ''Let's think step-by-step.'' \hfill \textbf{Score}: 70.3\;
    }
    \hlbox{ctuorange}{\textbf{Optional:} Examples of the task like in \ref{box:insinductexample} }
    }
    \textbf{LLM Output:}
    \hlbox{ctublue}{Plan and solve the challenge while showing your work.}
\end{promptbox}

\newpage

In \ref{sec:inference} we discussed the LLM's ability to self-refine, which works best with access to an extrinsic information source.
With feedback-based PO, the LLM outputs for each prompt - combined with the LLM's human-like task comprehension\cite{xiang2025selfsupervisedpromptoptimization} -
represent a rich source of quality information for prompt refinement. 

\subsubsection{Evolution-based approaches}

\subsubsection{Other approaches}




Meta-prompts are flexible but studies lack principled guidelines about their design. \cite{tang2024unleashingpotentiallargelanguage}

Analogical momentum forms inspired by the momentum method involve including the optimization trajectory in the meta-prompt. 
To fit into the context limit and reduce noise, trajectory can be summarized or k most recent/relevant/important gradients can be retrieved. \cite{tang2024unleashingpotentiallargelanguage}

To mimic effects or learning rate, prompt variation can be limited by edit distance (maximum words to be changed). 
Warm-up and decay strategies can be applied to this constraint. \cite{tang2024unleashingpotentiallargelanguage}

New prompt can be created by editing a previous prompt or generate a new one by following a demonstration. \cite{tang2024unleashingpotentiallargelanguage}

In an experiment on BBH, authors found that optimization without reflection performs better and the best momentum method being relevance. 
For prompt variation control, the best combination was cosine decay and no warm-up.  \cite{tang2024unleashingpotentiallargelanguage}

Summarization-based trajectory is less helpful because it tends to only capture common elements. \cite{tang2024unleashingpotentiallargelanguage}

Task input-output examples are beneficial in the meta-prompt to provide additional context to the LLM to understand the task. \cite{tang2024unleashingpotentiallargelanguage}

GPT-4 can consistently find better task prompts than GPT-3.5-turbo, which suggests the need for a capable model as the prompt optimizer \cite{tang2024unleashingpotentiallargelanguage}

Trajectory-based methods perform very well possible because trajectory 
helps the prompt optimizer pay more attention to the important information instead of the noise in the current step. \cite{tang2024unleashingpotentiallargelanguage}



Prompt to the LLM optimizer is called the meta-prompt and includes previous prompts with their 
training accuracies sorted in ascending order along with the task description and training set samples. \cite{yang2024largelanguagemodelsoptimizers}

The main advantage of LLMs for optimization is their ability of understanding natural language, 
which allows people to describe their optimization tasks without formal specifications. \cite{yang2024largelanguagemodelsoptimizers}

Motivated by linear regression and TSP and on small-scale traveling salesman problems, OPRO performs on par with some hand-crafted heuristic algorithms. \cite{yang2024largelanguagemodelsoptimizers}

Optimization stability can be improved by generating multiple solutions when relying on random ICL samples. \cite{yang2024largelanguagemodelsoptimizers}

To balance between exploration and exploitation, LLM sampling temperature can be tuned. Lower temperature encourages exploitation in the local solution space and higher temperature allows more aggressive exploration of different solutions. \cite{yang2024largelanguagemodelsoptimizers}

Only the top instructions are kept in the meta-prompt to fit in the LLM context limit. \cite{yang2024largelanguagemodelsoptimizers}

New outstanding solution is usually found only all the prompts are of similar quality: first all the worse prompts are purged and substituted by a prompt similar to the current best. \cite{yang2024largelanguagemodelsoptimizers}






\textbf{Symbolic search}
With increasing complexity of prompt structure, many prompt optimization techniques are no longer applicable. \cite{schnabel2024symbolicpromptprogramsearch}
Symbolic prompt programs (SPPs) can be represented as directed acyclic graphs where nodes are functions (subprograms) and edges indicate call dependencies. \cite{schnabel2024symbolicpromptprogramsearch}

Search space can be defined in two ways, as an enumerative search, where a small number of options is known beforehand, 
and iterative search, where a large search space is explored with iterative search strategies. \cite{schnabel2024symbolicpromptprogramsearch}



\subsubsection{Textual gradients}
Naturally there are no gradients in the text space but some researchers try to emulate them using reflection-based operators.

APO mirrors the steps of gradient descent within a text-based Socratic dialogue substituting differentiation with LLM feedback and backpropagation with LLM editing \cite{pryzant2023automaticpromptoptimizationgradient}

Beam search is an iterative optimization process where in current prompt is expanded into many more candidates in each iteration and a selection process decides which will be used in the next iteration. \cite{pryzant2023automaticpromptoptimizationgradient}

Expansion first uses gradients to edit the current prompt and then explores the local monte-carlo search space by paraphrasing the editions \cite{pryzant2023automaticpromptoptimizationgradient}

To limit the computation used on evaluating prompts, an approach inspired by best arm identification in bandit optimization is utilized. \cite{pryzant2023automaticpromptoptimizationgradient}


Applying previous iterative prompt optimization methods, based on prompt+score pairs, to text generation tasks is challenging due to the lack of effective optimization signals. \cite{he2024crispomultiaspectcritiquesuggestionguidedautomatic}

Critiques and suggestions, written in natural language, are more helpful for prompt improvement than a single score.\cite{he2024crispomultiaspectcritiquesuggestionguidedautomatic}

CriSPO uses prompt+score+critique triples for next candidate generation. \cite{he2024crispomultiaspectcritiquesuggestionguidedautomatic}
Unlike APE \cite{pryzant2023automaticpromptoptimizationgradient} prompt generation is decoupled from suggestions and a history of critiques and suggestions as packed into the optimizer for a more stable optimization. \cite{he2024crispomultiaspectcritiquesuggestionguidedautomatic}

CoT is applied in optimization by first asking to compare high-score prompts to low-score ones and draft general ideas. \cite{he2024crispomultiaspectcritiquesuggestionguidedautomatic}

Critique-based optimization explores a larger space, which is indicated by lower similarity of the prompts in lexicons and semantics.\cite{he2024crispomultiaspectcritiquesuggestionguidedautomatic}

CriSPO outperforms OPRO \cite{yang2024largelanguagemodelsoptimizers} both on summarization and QA tasks and metaprompt allows for creating ICL and RAG template prompts. \cite{he2024crispomultiaspectcritiquesuggestionguidedautomatic}

\textbf{DSPy optimizers}

Most prompt optimizer approaches do not apply to multi-stage LLM programs where we lack gold labels or evaluation metrics for individual LLM calls. \cite{opsahlong2024optimizinginstructionsdemonstrationsmultistage}


Uses a surrogate Bayesian optimization model, which is updated periodically by evaluating the program on batches, to sample instructions and demonstrations for each stage of the LLM program \cite{opsahlong2024optimizinginstructionsdemonstrationsmultistage}

Optimizing demonstrations alone usually yields better performance than just optimizing instructions, but optimizing both yield the best performance. \cite{opsahlong2024optimizinginstructionsdemonstrationsmultistage}

Optimizing instructions is most valuable for tasks with subtle conditional rules not expressible by a few examples.  \cite{opsahlong2024optimizinginstructionsdemonstrationsmultistage}

For LLM programs, it is beneficial to alternate between optimizing weights (fine-tuning) and optimizing prompts. \cite{soylu2024finetuningpromptoptimizationgreat}

\subsubsection{Evolutionary optimization}
There is considerable synergy potential between the fields of evolutionary computation and deep learning. \cite{lehman2022evolutionlargemodels}

\textbf{LLM-based variation operator}

LLMs trained on code can suggest intelligent mutations and thus sidestepping many of the challenges in evolving programs. \cite{lehman2022evolutionlargemodels}

Genetic programming still offers an advantage when the programming task is far from the training distribution of the LLM. \cite{lehman2022evolutionlargemodels}

Genetic programming can in principle evolve in any space. \cite{lehman2022evolutionlargemodels}

\todo{this can be connected with the idea that prompts=programs}

Topic of mutation is guided by previously chosen "commit message" \todo{this is like a pseudogradient}. \cite{lehman2022evolutionlargemodels}

Evolution with language models can be used as a way of generating domain data for downstream deep learning where it did not previously exist. \cite{lehman2022evolutionlargemodels}


The pattern-completion ability of few-shot prompting can be leveraged to create a form of intelligent evolutionary crossover. \cite{meyerson2024languagemodelcrossovervariation}

Performance at in-context learning improves with model scale, implying that methods relying upon this capability will benefit from continuing progress in LLM training. \cite{meyerson2024languagemodelcrossovervariation}

Advent of large, pretrained foundation models marked a significant step in the paradigm of evolutionary 
recombination with deep generative models, where these new models can be directly leveraged without additional training and allow for searching more abstract spaces. \cite{meyerson2024languagemodelcrossovervariation}

Next-token prediction naturally lends itself to creating an evolutionary variation operator. \cite{meyerson2024languagemodelcrossovervariation}

LLM variation operator should be capable of generating meaningful variation for any text representation that has moderate support in the training set, meaning it is basically domain-independent. \cite{meyerson2024languagemodelcrossovervariation}

LLM crossover acts like an EDA in that it builds a probabilistic model of parents from which the children are sampled. \cite{meyerson2024languagemodelcrossovervariation}

LLM crossover becomes more similar to an EDA as the number of parents increases. \cite{meyerson2024languagemodelcrossovervariation}

LLM crossover can express any genetic operator even with small parent sets by fine-tuning or through effective prompting schemes. \cite{meyerson2024languagemodelcrossovervariation}

Simultaneously evolving the solution $\mathit{x}$ along with the LLM and the prompting mechanism could be a powerful paradigm for more open-ended systems. \cite{meyerson2024languagemodelcrossovervariation}

Input arranged in ascending fitness order prompts the model to generate output that follows an ascending fitness trend. \cite{meyerson2024languagemodelcrossovervariation}

Evolutionary algorithms have been shown to be effective in search spaces with millions and billions of variables, which are inaccessible to LLM crossover due to the size of the LLM context window. \cite{meyerson2024languagemodelcrossovervariation}

The level of stochasticity in LMX can be controlled by the softmax temperature parameter, which can be seen as analogous to a mutation rate parameter in a traditional EA \cite{meyerson2024languagemodelcrossovervariation}


\textbf{Frameworks}
Building upon the inherent ability of LLMs to paraphrase (mutation) and 
combine (crossover) text, an interesting intersection of traditional evolutionary algorithms and modern LLMs has formed. 


Sequences of phrases can be regarded as gene sequences in 
typical Evolutionary algorithms. \cite{guo2024connectinglargelanguagemodels}


Considers two widely used EAs: Genetic Algorithm and 
Differential Evolution with DE outperforming GA on most tasks \cite{guo2024connectinglargelanguagemodels}

ELM uses a MAP-elites Quality Diversity algorithm. \cite{lehman2022evolutionlargemodels}

Initial population consists of manually-written prompts to leverage human knowledge as well as some prompts generated by LLMs to reflect the fact that EAs start from random solutions to avoid local optima. \cite{guo2024connectinglargelanguagemodels}

DE-inspired approached builds on the idea that the common elements of the current best prompts need to be preserved \cite{guo2024connectinglargelanguagemodels}

Evoprompt performs best with roulette selection when compared with tournament and random selection. \cite{guo2024connectinglargelanguagemodels}

Similar results are achieved when population is initialized with the best and with random prompts, hinting that the crafted design of initial prompts is not essential. \cite{guo2024connectinglargelanguagemodels}


Previous research optimized zero-shot instructions and examples separately, overlooking their interplay and resulting in sub-optimal performance. \cite{cui2024phaseevounifiedincontextprompt}

There is a prevailing notion that prompt engineering sacrifices efficiency for performance due to the lengthening of prompts, but PhaseEvo actively shortens the prompts \cite{cui2024phaseevounifiedincontextprompt}

Current EA applications to prompt optimization suffer from extremely high computational cost and slow convergence speed due to the complexity of the high-dimensional search space. \cite{cui2024phaseevounifiedincontextprompt}

PhaseEvo alternates between two phases: exploration with evolution operators and exploitation using a feedback "gradient". \cite{cui2024phaseevounifiedincontextprompt}

TABLE 1 \todo{recreate} compares all 5 operators.  \cite{cui2024phaseevounifiedincontextprompt}

4 phases: initialization - lamarck or manual, local feedback mutation, global evolution with EDA and CR operators, local semantic mutation (paraphrasing) \cite{cui2024phaseevounifiedincontextprompt}

Candidates for evolution operators are selected based on a "performance vector", combining prompts that do not make the same mistakes.  \cite{cui2024phaseevounifiedincontextprompt}

When the performance improvement with an operator stagnates up to some operator-specific tolerance, the current phase is terminated. \cite{cui2024phaseevounifiedincontextprompt}

Evolution in phases outperforms random operator selection. \cite{cui2024phaseevounifiedincontextprompt}

PhaseEvo is the most cost-effective but still needs around 12 iterations and 4000 API calls. \cite{cui2024phaseevounifiedincontextprompt}


APE \cite{zhou2023largelanguagemodelshumanlevel} ran into problems with diminishing returns and abandoning the iterative approach entirely, Promptbreeder aims to solve this with a diversity-maintaining evolutionary algorithm for self-referential self-improvement of prompts \cite{fernando2023promptbreederselfreferentialselfimprovementprompt}

Prompt optimization techniques utilize the fact that LLMs are effective at generating mutations from examples and can encode human notions of interestingness and can be used to quantify novelty. \cite{fernando2023promptbreederselfreferentialselfimprovementprompt}

Self-referential system should improve the way it is improving, thus Promptbreeder used a "hyper-prompt" to optimize its meta-prompt \cite{fernando2023promptbreederselfreferentialselfimprovementprompt}

Uses a binary tournament genetic algorithm. \cite{fernando2023promptbreederselfreferentialselfimprovementprompt}

Uses a random uniformly sampled mutation operators out of 9 total from 5 broad categories for each replication event. \cite{fernando2023promptbreederselfreferentialselfimprovementprompt}

Zero-order mutation (creating a prompt from task description) generates new task prompts more aligned with the task description in the event the evolution diverges.  \cite{fernando2023promptbreederselfreferentialselfimprovementprompt}

LLMs tend to be biased to examples found later in EDA mutation lists. Lying to the LLM and telling it that the prompts are sorted by performance in a descending order improves diversity.  \cite{fernando2023promptbreederselfreferentialselfimprovementprompt}

Removing any self-referential operator in ablation is harmful under nearly all circumstances \cite{fernando2023promptbreederselfreferentialselfimprovementprompt}


Prior works do not provide sufficient guidance in the meta-prompt. \cite{ye2024promptengineeringpromptengineer}

PE2 improves on APE and APO with a back-tracking search procedure and a more complete metaprompt with a two-step task description, prompt layout specification and a step-by-step reflection template. \cite{ye2024promptengineeringpromptengineer}

Experiments with step size and momentum and also a prompt engineering tutorial in the metaprompt to mixed results. \cite{ye2024promptengineeringpromptengineer}

Optimized prompts do not seem to be generalizable across different models.  \cite{ye2024promptengineeringpromptengineer}


\textbf{Gold-agnostic methods}

Current prompt optimization methods often depend heavily on external references for evaluation which are often unavailable or unpractical to define in many applications, especially for open-ended tasks \cite{xiang2025selfsupervisedpromptoptimization}
Gold-agnostic evaluation is important because we ultimately expect LLMs to solve problems for which answers are not already known \cite{zhang2024glapegoldlabelagnosticprompt}

Two primary sources can be used for evaluation: LLM-generated outputs and task-specific truths. These can then be evaluated using either a predefined metric, LLM-as-a-judge or by a human judge to produce an optimization signal based on a numeric score or a textual feedback.  \cite{xiang2025selfsupervisedpromptoptimization}

In each iteration, SPO generates new prompts, executes them, and performs pair-wise evaluations of outputs to assess their adherence \cite{xiang2025selfsupervisedpromptoptimization}

SPO achieves high efficiency, requiring only 8 LLM calls per iteration with three samples, significantly lower than existing methods \cite{xiang2025selfsupervisedpromptoptimization}

Outputs of LLMs inherently contain rich quality information that directly reflects prompt effectiveness \cite{xiang2025selfsupervisedpromptoptimization}

LLMs exhibit human-like task comprehension \cite{xiang2025selfsupervisedpromptoptimization}

With score-based feedback a large sample size is needed to ensure scoring stability, which can be avoided by pairwise comparison of outputs \cite{xiang2025selfsupervisedpromptoptimization}

LLM-as-a-judge biases do not affect the overall optimization trend because eval’s feedback merely serves as a reference for the next round of optimization \cite{xiang2025selfsupervisedpromptoptimization}

While maintaining comparable performance with other ground truth-dependent prompt optimization methods, SPO requires only 1.1\% to 5.6\% of their optimization costs \cite{xiang2025selfsupervisedpromptoptimization}



Self-consistency can by used as a metric instead of accuracy as correct answers generally exhibit greater self-consistency than incorrect ones. However it can overestimate prompts that produce consistent incorrect answers. \cite{zhang2024glapegoldlabelagnosticprompt}

Mutual-consistency refinement refines self-consistency scores based on the self-consistency scores of other prompts \cite{zhang2024glapegoldlabelagnosticprompt}

Gold-agnostic evaluation methods like GLaPE are robust metrics akin to accuracy and are able to produce effective prompts similarly to gold-label-based methods like OPRO\cite{yang2024largelanguagemodelsoptimizers}. \cite{zhang2024glapegoldlabelagnosticprompt}

If all prompts produce consistent but incorrect answers it is challenging to discern the error without external resources. This happens in some datasets, leading to diminished correlation between GLaPE and accuracy. \cite{zhang2024glapegoldlabelagnosticprompt}


\subsubsection{Metaprompting}
Metaprompting or "prompting to create prompts"

Meta-prompts are task-agnostic, meaning they will return the relevant outputs for an arbitrary task, provided a task description is provided as an input.  \cite{dewynter2024metaprompting}.

It is possible to construct a general-purpose meta-prompt. \cite{dewynter2024metaprompting}.

Meta-prompts will perform better than standard prompts at executing a wide range of tasks.  \cite{dewynter2024metaprompting}.

In tests meta-generated prompts were ranked as more useful than the baselines as well as producing content that was rated more suitable.  \cite{dewynter2024metaprompting}.

\subsection{General LLM-based optimization}