\section{Inference-time scaling}
Inference-time scaling or test-time scaling is a paradigm that has gained traction in the recent years
with the advent of dedicated reasoning models \todo{cite some model cards / deepseek}. 
As opposed to training-time scaling, where the performance of models scales with 
training times, model parameter counts and dataset sizes \todo{cite smth about training scaling},
inference-time scaling aims to improve performance by dedicating more resources to each inference call.

At their heart, LLMs are probabilistic models over sequences and to generate a sequence, they employ generation algorithms. 
Welleck et al.\cite{welleck2024decodingmetagenerationinferencetimealgorithms} provide an overview of these generation algorithms
and then frame more advanced inference-time techniques as meta-generations, or strategies that employ sub-generators.
Most generation algorithms attempt to find either highly probable sequences (MAP algorithms) or sample from the model's distribution.
The simplest MAP algorithm is greedy decoding, which recursively finds the next token with the highest probability in the distribution.

A generalization of greedy decoding is the beam search algorithm which maintains a structure of possible prefixes and each step expands them and scores them.
An example of a beam search algorithm\cite{wang2024chainofthoughtreasoningprompting} can identify decoding branches where the model 
employs a reasoning chain to solve a given task. Authors of this algorithm found that answer tokens found in the decoding paths with a reasoning chains 
have greater token probabilities, meaning the model shows greater confidence in its answer having reasoned about it beforehand.
In general beam search improves on simple greedy decoding but at a high computational cost.

An interpolation between greedy decoding and uniform sampling is temperature sampling, which 
outperforms other adapters in input-output tasks like code generation and translation. 
An example of algorithms that sample from the model's distribution is the ancestral sampling algorithm.
Interpolating between ancestral sampling and simple greedy sampling gave rise to decding algorithms such as
nucleus, top-k and $\eta$- and $\epsilon$-sampling. When we require a structured output, for example a JSON data 
structure following a JSON schema, we can utilize parser-based decoding, which enforce a structural requirement.
This can however come at worsened performance when using inflexible templates.

These strategies can be divided into the categories of chained, parallel, step-level, and refinement-based meta-generators\cite{welleck2024decodingmetagenerationinferencetimealgorithms}.

\subsection{Chained meta-generation}

Chained meta-generation is the composition of several subgenerators in sequence. 
These can be LLM calls or other functions that use previous inputs, such as code execution function \todo{cite program of thought}.
The subgenerators can be implemented as several LLM calls or with a single call given sufficient instructions in the prompt. \cite{khattab2023dspycompilingdeclarativelanguage}
Some examples include Program-of-thought, Plan-and-Solve and Chain-of-Thought techniques.

\subsubsection{Chain-of-thought (CoT)}
Chain-of-Thought (CoT) is a LLM prompting technique that works by inducing a coherent series of intermediate 
reasoning steps that lead to the final answer for a problem\cite{wei2023chainofthoughtpromptingelicitsreasoning}.
Existing work suggest LLMs falter in a direct-QA scenarios (without inducing CoT), where the greedy decoding path mostly does not contain a reasoning chain\cite{wang2024chainofthoughtreasoningprompting}. 
In its essence, the model is a left-to-right text completion engine. We can make the analogy with human thinking 
modes, where it is said that humans have a fast automatic "System 1" mode and a slow and deliberate "System 2" mode\cite{yao2023treethoughtsdeliberateproblem}. 
In direct-QA mode, the LLM can underestimate the difficulty of the task\cite{wang2024chainofthoughtreasoningprompting} and stay in the "System 1" thinking mode.
By crafting a good prompt that instructs the model to reason we can shift the model from "System 1" to "System 2" thinking.
Furthermore, CoT allows models to allocate additional computation to problems with more reasoning steps\cite{wei2023chainofthoughtpromptingelicitsreasoning}
Prystawski et al.\cite{prystawski2023thinkstepstepreasoning} also speculate that direct prediction fails for tasks where
the relevant variables are rarely seen together in training, whereas CoT reasoning can incrementally chain known dependencies. 


CoT can been elicited by prompting techniques - few-shot with steps demonstrations or 
zero-shot with specific instructions\cite{wang2024chainofthoughtreasoningprompting}
First CoT methods\cite{wei2023chainofthoughtpromptingelicitsreasoning} involved one/few-shot prompting, 
where the prompt included examples of CoT reasoning in the prompt in facilitate a reasoning chain response.
Although effective, this requires human engineering of multi-step reasoning prompts.
This method is also highly sensitive to prompt design with performance deteriorating 
for mismatched prompt example and task question types\cite{NEURIPS2022_8bb0d291}.
For this method, authors found that CoT is an emergent capability of model scale 
and did not observe benefits for small models\cite{wei2023chainofthoughtpromptingelicitsreasoning}.

On the other hand, zero-shot prompting can induce a reasoning chain with a simple prompt like "Let's think step-by-step",
making it versatile and task-agnostic\cite{NEURIPS2022_8bb0d291}. Similar prompts also improve reasoning performance and 
some research\todo{tady OPRO? nebo kde hledali cot prefixy} has been been done on finding the optimal CoT prefix prompt.

Apart from prompting, CoT can been elicited by model training or tuning. 
This method, requiring a significant amount of reasoning data\cite{wang2024chainofthoughtreasoningprompting},
has gained traction with the development of dedicated reasoning models like OpenAI's o1 or Deepseek-R1\todo{cite o1 deepseek}.
Using methods such as supervised fine-tuning (SFT) or reinforcement learning (RL), the model is trained to
automatically produce longer reasoning chains, often bound in dedicated "thought" tags or tokens. These models have shown significant performance boosts on reasoning benchmarks \todo{cite}.
Models similar to o1 all primarily extend solution length by self-revision\cite{zeng2025revisitingtesttimescalingo1like}
After finishing a thought process, the model tries to self-revise, which is marked by words such as "Wait" or "Alternatively". 
The model then tries to spot mistakes or inconsistencies in its reasoning or propose an alternative solution. 
Self-revision ability is thus a key factor in the effectiveness of sequential scaling for reasoning models. \cite{zeng2025revisitingtesttimescalingo1like}

Prompting techniques like chain-of-thought can increase answer quality 
at the cost of longer and more computationally expensive outputs. \cite{brown2024largelanguagemonkeysscaling}
Performance gains are observed mainly on arithmetic and coding tasks with more performance gains 
being observed for more complicated problems\cite{wei2023chainofthoughtpromptingelicitsreasoning}.
Further research by Liu et al.\cite{liu2024mindstepbystep} suggests that for some tasks CoT can be detrimental.
Their experiments proved their hypothesis that CoT hurts performance on tasks where humans do better without deliberation
and where the nature of LLM, like the much greater context memory, does not provide an advantage over human thinking. 
This phenomenon was observed on tasks like facial recognition, implicit statistical learning or pattern recognition.
Limited performance gains were noticed on commonsense reasoning tasks\cite{NEURIPS2022_8bb0d291}.

Longer reasoning chains mean more computing power spent at inference. How far can we take this sequential scaling?
In their study, Zeng at al.\cite{zeng2025revisitingtesttimescalingo1like} argue that longer CoTs do not consistently improve accuracy of reasoning models.
Furthermore, they find that the average length of correct solutions is shorter than that of incorrect ones. 
Because self-revision accounts for most of the CoT length, the effectiveness of the method relies on the model's ability to self-revise.
Authors of this paper argue that the self-revision ability of models is insufficient as they demonstrate limited capacity to correct their answers
during self-revision. Some models on some tasks are even more likely to change a correct answer to an incorrect one than vice-versa.

\subsection{Parallel meta-generation}

Parallel meta-generation involves multiple generations concurrently. 
The final answer can then be chosen - with a reward model or with 
voting - or constructed from the ensemble of generations \cite{welleck2024decodingmetagenerationinferencetimealgorithms}.

One of the simplest such techniques is self-consistency\cite{wang2023selfconsistencyimproveschainthought} (SC),
a method which builds upon CoT to aggregate answers from diverse reasoning 
chains and selects the best one based on majority voting. 
It significantly improves accuracy in a range of arithmetic and commonsense reasoning tasks \cite{wang2023selfconsistencyimproveschainthought}.
The effectiveness of SC comes from the fact that, for tasks with objective answers, there are more ways to be right than wrong.
For our next discussion of SC and related methods we will compare the 
terms \textit{coverage} $\mathrm{C}_{\mathbb{D}}$ and \textit{accuracy} $\mathrm{A}_{\mathbb{D}}$ for a dataset ${\mathbb{D}}$.
Given a language model $\mathcal{M}$, a task query $q_k \in {\mathbb{D}}$ and a task 
instruction $\mathbf{i}$, we can define the generation collection of length $n$ as
\begin{equation}
    Y_k = \{y_{jk}\mid j \in 1, ..., n\},
\end{equation}
\begin{equation}
    y_{jk} \sim \mathcal{M}(\mathbf{i}(q_k)).
\end{equation}
For objective tasks we can check the correctness with a metric $\mathcal{G}$
\begin{equation}
    \mathcal{G}_{k}(y_{jk}, q_k) = 
    \begin{cases}
        1.0 & y_{jk} \text{ is the correct answer for } q_k\\
        0.0 & y_{jk} \text{ is an incorrect answer for } q_k.
    \end{cases}
\end{equation}
To choose the final answer, we will define a answer selection function $\mathcal{S}(Y)$. 
This can be a majority vote selection function or some reward-based method.
We can now define \textit{coverage} $\mathrm{C}_{\mathbb{D}}$ and \textit{accuracy} $\mathrm{A}_{\mathbb{D}}$ as
\begin{align}
    \mathrm{C}_{\mathbb{D}} &= \frac{1}{|\mathbb{D}|} \sum_{q_k \in \mathbb{D}} \max_{j=1,...,n} \mathcal{G}_k(y_{jk}, q_k) \\
    \mathrm{A}_{\mathbb{D}} &= \frac{1}{|\mathbb{D}|} \sum_{q_k \in \mathbb{D}} \mathcal{G}_k\left( \mathcal{S}(Y_k), q_k \right).
\end{align}



Works with both few-shot and zero-shot CoT. \cite{wang2023selfconsistencyimproveschainthought}
One limitation of self-consistency is that it incurs more computation cost. 
Small number of paths is enough, as in most cases the performance saturates quickly. \cite{wang2023selfconsistencyimproveschainthought}




Generating large sample collections is only useful if the correct samples in a collection can be identified. \cite{brown2024largelanguagemonkeysscaling}

It is possible and sometimes cost-effective, to amplify weaker models with many samples and outperform 
single samples from more capable models. \cite{brown2024largelanguagemonkeysscaling}

Relationship between coverage and the number of samples can often be modeled using an exponentiated power law, suggesting a form of scaling laws for inference-time compute although not as exact as training scaling laws. \cite{brown2024largelanguagemonkeysscaling}

Repeated sampling can make use of high batch sizes and specialized optimizations that improve 
system throughput relative to single-attempt inference workloads. \cite{brown2024largelanguagemonkeysscaling}

Coverage and precision diverge as number of samples increases on tasks without automatic verifiers, 
highlighting the need for improving sample verification methods. \cite{brown2024largelanguagemonkeysscaling}


All parallel scaling methods rely on guidance signals to select the optimal token, step, or solution from a set of candidates. \cite{zeng2025revisitingtesttimescalingo1like}

For the same number of generated tokens, parallel scaling provides a significantly larger improvement in coverage compared to sequential scaling. \cite{zeng2025revisitingtesttimescalingo1like}

Practical parallel scaling method must select a final answer from a set of candidate answers. \cite{zeng2025revisitingtesttimescalingo1like}

Shortest majority vote takes into account the fact that correct solution chains are shorter on average and extends Majority vote by weighing solution counts with the log of average solution length for given solution category, outperforming Majority voting on AIME. \cite{zeng2025revisitingtesttimescalingo1like}

\subsection{Step-level meta-generation}
Step-level meta-generation implements search algorithms on the generation state-space, which can be made up of tokens or longer sequences. Many search algorithms and state evaluation functions are possible. \cite{welleck2024decodingmetagenerationinferencetimealgorithms}


Multi-turn inference-time methods with a fixed width exhibit diminishing gains when computational budget is increased, failing to leverage the vast output space of LLMs.\cite{misaki2025widerdeeperscalingllm} 

Unlike the standard tasks typically tackled by tree search algorithms where the number of possible actions at each node is finite, each call to LLM can yield a new output even for the same input, making each node’s branching factor theoretically infinite.\cite{misaki2025widerdeeperscalingllm} 


\subsubsection{Tree-of-thought}
Similar to CoT with self-consistency but the reasoning chain is split up into steps creating a tree. This reasoning tree can then be explored using a graph search algorithm such as DFS.


\subsection{Refinement meta-generation}
Refinement meta-generation generates a revised version of the output based on past versions and additional information such as intrinsic or extrinsic feedback or environment observations. \cite{welleck2024decodingmetagenerationinferencetimealgorithms}
For extrinsic refinement, it is plausible that there are information sources which add new information, and hence lead to a potential gain with refinement but the efficacy of intrinsic refinement has been mixed. \cite{welleck2024decodingmetagenerationinferencetimealgorithms}


Inference-Time Scaling by training models to think before responding is insufficient because these methods include Reinforcement learning with verifiers, making them unsuitable for open-ended tasks. \cite{wang2025dedicatedfeedbackeditmodels}

Authors train dedicated Feedback and Edit models that can be used at inference time to improve model responses to open-ended general domain tasks. \cite{wang2025dedicatedfeedbackeditmodels}

Efficacy of LLMs in providing feedback and making edits to their own responses is unclear as using LLMs that were not specifically trained to provide feedback is ineffective compared to using high quality feedback. \cite{wang2025dedicatedfeedbackeditmodels}


\subsubsection{Reflexion}
Reflexion converts binary or scalar feedback from the environment into verbal feedback in the form 
of a textual summary, which is then added as additional context for the LLM agent, e.g. CoT or ReAct module, in the next episode. \cite{shinn2023reflexionlanguageagentsverbal}

The model can go through several steps of using the tools, which generates "observation". The model uses these observations to generate a final answer and leaves the ReAct chain when ready using a "finish" function.
Reflections go into a long-term memory context limited to a sliding window with maximum capacity. \cite{shinn2023reflexionlanguageagentsverbal}

Improves performance over strong baselines on sequential decision making, reasoning and programming tasks. \cite{shinn2023reflexionlanguageagentsverbal}
\subsubsection{ReAct}
Multi-turn prompting technique that forms the basis of agentic LLMs. The model is given a set of tools, such as a Wikipedia search function or a math expression evaluator.


\section{Prompting techniques}

Prompting techniques encode human priors, making it difficult to assess a language model's intrinsic reasoning abilities \cite{wang2024chainofthoughtreasoningprompting}

\subsection{Prompt engineering}
In many modern LLM applications, prompts have become programs themselves. \cite{schnabel2024symbolicpromptprogramsearch}
Motivation for prompt engineering is to improve the model's capabilities not by changing the underlying weights with training on data but by crafting an optimal instruction string, or a prompt.
This can be done by providing examples of the task as a part of the prompt or by instructing the model how to solve the task.

\subsubsection{In-context learning}
Prompts are distinguished based on the number of included examples.
\begin{table}[h!]
    \centering
    \begin{tabular}{|c|p{12cm}|}
    \hline
    \textbf{Prompting Type} & \textbf{Description} \\
    \hline
    Zero-shot Prompting & Prompt has no examples, model relies on its pre-trained knowledge. \\
    \hline
    One-shot Prompting & Prompt has one example to guide the model. \\
    \hline
    Few-shot Prompting & Prompt includes a few examples (typically 2 to 5). \\
    \hline
    \end{tabular}
    \caption{Comparison of Zero-shot, One-shot, and Few-shot Prompting}
\end{table}        
Research\cite{brown2020languagemodelsfewshotlearners} has shown that with growing model size the knowledge-generalizing ability of the model increases. Instead of expensive fine-tuning
models can reuse knowledge from pre-training and solve many tasks when provided just by a few examples.

Few-shot prompting highlights that LLMs can be seen as powerful pattern-completion engines. \cite{meyerson2024languagemodelcrossovervariation}

Providing a prompt of examples from a distribution can condition the LLM to generate further high-probability examples from that distribution \cite{meyerson2024languagemodelcrossovervariation}

\subsection{Prompting techniques}
\todo{talk about how we can achieve meta-generation just by updating the prompt}
